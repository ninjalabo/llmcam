{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4d8c4c4-0bc0-488b-a1a4-edd3c47481f6",
   "metadata": {},
   "source": [
    "# OAS to Tool Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8257ad-690a-4ec4-93d4-c78dc561401f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core.oas_to_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433e912d-60a9-4e23-a9d5-35f78140cbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b7918e-55d3-4bfa-a5fb-05f9513b989a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import copy\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import yaml\n",
    "from typing import Callable, Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12becbb4",
   "metadata": {},
   "source": [
    "OpenAPI Specification is a standardized format used to describe and document RESTful APIs. The OAS format enables the generation of both human-readable and machine-readable API documentation, making it easier for developers to design, consume, and integrate with APIs. \n",
    "\n",
    "In this context, we can also utilize this for GPT function calling by generating tools schema from this OAS file and implementing a wrapper function to send requests to API server. This implementation focuses on DigiTraffic with its OpenAPI specification file. Summary of the generation process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a33fa51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"display: flex; justify-content: center; align-items: center; width: 100%; height: 100%;\">\n",
       "        <img src=\"https://mermaid.ink/img/CmZsb3djaGFydCBURAogICAgQVtBUEldIC0tPiBPQVNbT3BlbkFQSSBTcGVjaWZpY2F0aW9uXQogICAgT0FTIC0tPnxkb3dubG9hZCAmIHBhcnNlfCBPQVNfRElDVFtEYXRhIGluIFB5dGhvbiBEaWN0aW9uYXJ5XQogICAgT0FTX0RJQ1QgLS0-fGRlZXAgcmVmZXJlbmNlIGV4dHJhY3Rpb258IEZFW0ZsYXR0ZW5lZCAvIGV4cGFuZGVkIGRhdGEgbW9kZWxzXQogICAgRkUgLS0-fGNvbnZlcnR8IENGRVtHUFQtY29tcGF0aWJsZSBtb2RlbHNdCiAgICBPQVNfRElDVCAtLT58cGFyc2V8IEVDW0VuZHBvaW50IGNvbmZpZ3VyYXRpb25zXQogICAgRUMgLS0-IFBFQ1tFbmRwb2ludCBwYXJhbWV0ZXJzXQogICAgUEVDQHsgc2hhcGU6IGJyYWNlcywgbGFiZWw6ICJQYXJhbWV0ZXJzOiBQYXRoIC8gUXVlcnkgLyBSZXF1ZXN0IEJvZHkiIH0KICAgIENGRSAtLT58YXR0YWNofCBQRUMKICAgIEVDIC0tPnxvcGVyYXRpb24gSUQgLyBnZW5lcmF0ZXwgRk5bRnVuY3Rpb24gbmFtZV0KICAgIEVDIC0tPiBNRFtNZXRhZGF0YV0KICAgIE1EQHsgc2hhcGU6IGJyYWNlcywgbGFiZWw6ICJNZXRhZGF0YTogVVJMICYgTWV0aG9kIiB9CiAgICBQRUMgLS0-IFNbU2NoZW1hXQogICAgRk4gLS0-IFMKICAgIE1EIC0tPiBTCiAgICBGVVtGaXh1cCBmdW5jdGlvbiB0aGF0IGdlbmVyYXRlIHJlcXVlc3RzIHRvIG9yaWdpbmFsIEFQSV0gLS0-IFMK\" style=\"max-width: 100%; max-height: 100%; object-fit: contain;\" />\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| echo: false\n",
    "import base64\n",
    "from IPython.display import HTML, display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def mm(graph):\n",
    "    graphbytes = graph.encode(\"utf8\")\n",
    "    base64_bytes = base64.urlsafe_b64encode(graphbytes)\n",
    "    base64_string = base64_bytes.decode(\"ascii\")\n",
    "    img_url = \"https://mermaid.ink/img/\" + base64_string\n",
    "    \n",
    "    # Responsive HTML with CSS for fitting to parent container\n",
    "    html = f\"\"\"\n",
    "    <div style=\"display: flex; justify-content: center; align-items: center; width: 100%; height: 100%;\">\n",
    "        <img src=\"{img_url}\" style=\"max-width: 100%; max-height: 100%; object-fit: contain;\" />\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    display(HTML(html))\n",
    "\n",
    "mm(\"\"\"\n",
    "flowchart TD\n",
    "    A[API] --> OAS[OpenAPI Specification]\n",
    "    OAS -->|download & parse| OAS_DICT[Data in Python Dictionary]\n",
    "    OAS_DICT -->|deep reference extraction| FE[Flattened / expanded data models]\n",
    "    FE -->|convert| CFE[GPT-compatible models]\n",
    "    OAS_DICT -->|parse| EC[Endpoint configurations]\n",
    "    EC --> PEC[Endpoint parameters]\n",
    "    PEC@{ shape: braces, label: \"Parameters: Path / Query / Request Body\" }\n",
    "    CFE -->|attach| PEC\n",
    "    EC -->|operation ID / generate| FN[Function name]\n",
    "    EC --> MD[Metadata]\n",
    "    MD@{ shape: braces, label: \"Metadata: URL & Method\" }\n",
    "    PEC --> S[Schema]\n",
    "    FN --> S\n",
    "    MD --> S\n",
    "    FU[Fixup function that generate requests to original API] --> S\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c546f2",
   "metadata": {},
   "source": [
    "## Download OAS\n",
    "\n",
    "Many OAS files are available or can be easily generated from an API server. We can download such files and open them as Python dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff03b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def load_oas(\n",
    "    oas_url: str,  # OpenAPI Specification URL\n",
    "    destination: str,  # Destination file\n",
    "    overwrite: bool = False  # Overwrite existing file\n",
    ") -> dict:  # OpenAPI Specification\n",
    "    \"\"\"Load OpenAPI Specification from URL or file.\"\"\"\n",
    "    # Create destination directory if it does not exist\n",
    "    dirpath = os.path.dirname(destination)\n",
    "    if dirpath != \"\": os.makedirs(dirpath, exist_ok=True)\n",
    "\n",
    "    # Download OpenAPI Specification if it does not exist or overwrite is True\n",
    "    if not os.path.exists(destination) or overwrite:\n",
    "        r = requests.get(oas_url)\n",
    "        with open(destination, \"w\") as f:\n",
    "            f.write(r.text)\n",
    "\n",
    "    # Load OpenAPI Specification\n",
    "    with open(destination, \"r\") as f:\n",
    "        if destination.endswith(\".json\"):\n",
    "            return json.load(f)\n",
    "        elif destination.endswith(\".yaml\") or destination.endswith(\".yml\"):\n",
    "            return yaml.load(f)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid file format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79450099",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/ninjalabo/llmcam/blob/main/llmcam/utils/store.py#L19){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### load_oas\n",
       "\n",
       ">      load_oas (oas_url:str, destination:str, overwrite:bool=False)\n",
       "\n",
       "*Load OpenAPI Specification from URL or file.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| oas_url | str |  | OpenAPI Specification URL |\n",
       "| destination | str |  | Destination file |\n",
       "| overwrite | bool | False | Overwrite existing file |\n",
       "| **Returns** | **dict** |  | **OpenAPI Specification** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/ninjalabo/llmcam/blob/main/llmcam/utils/store.py#L19){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### load_oas\n",
       "\n",
       ">      load_oas (oas_url:str, destination:str, overwrite:bool=False)\n",
       "\n",
       "*Load OpenAPI Specification from URL or file.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| oas_url | str |  | OpenAPI Specification URL |\n",
       "| destination | str |  | Destination file |\n",
       "| overwrite | bool | False | Overwrite existing file |\n",
       "| **Returns** | **dict** |  | **OpenAPI Specification** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(load_oas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1756d92",
   "metadata": {},
   "source": [
    "Load OAS from Road DigiTraffic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db56306b-1233-491c-ae16-88ddef50c674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'get': {'tags': ['Beta'],\n",
       "  'summary': 'List the history of sensor values from the weather road station',\n",
       "  'operationId': 'weatherDataHistory',\n",
       "  'parameters': [{'name': 'stationId',\n",
       "    'in': 'path',\n",
       "    'description': 'Weather station id',\n",
       "    'required': True,\n",
       "    'schema': {'type': 'integer', 'format': 'int64'}},\n",
       "   {'name': 'from',\n",
       "    'in': 'query',\n",
       "    'description': 'Fetch history after given date time',\n",
       "    'required': False,\n",
       "    'schema': {'type': 'string', 'format': 'date-time'}},\n",
       "   {'name': 'to',\n",
       "    'in': 'query',\n",
       "    'description': 'Limit history to given date time',\n",
       "    'required': False,\n",
       "    'schema': {'type': 'string', 'format': 'date-time'}}],\n",
       "  'responses': {'200': {'description': 'Successful retrieval of weather station data',\n",
       "    'content': {'application/json;charset=UTF-8': {'schema': {'type': 'array',\n",
       "       'items': {'$ref': '#/components/schemas/WeatherSensorValueHistoryDto'}}}}},\n",
       "   '400': {'description': 'Invalid parameter(s)'}}}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oas = load_oas(\n",
    "    oas_url=\"https://tie.digitraffic.fi/swagger/openapi.json\",\n",
    "    destination=\"openapi.json\",\n",
    "    overwrite=False\n",
    ")\n",
    "oas['paths'][list(oas['paths'].keys())[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb0a4ef",
   "metadata": {},
   "source": [
    "## Deep reference extraction\n",
    "\n",
    "In order to generate tool schemas, we need to resolve and flatten the references to `components`. This process is complicated because of the nested references in `components`, which may also include circular references and lead to efficiency issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d44a6c3",
   "metadata": {},
   "source": [
    "To address these problems, the implemented algorithm involves:\n",
    "\n",
    "1. Traverse `components` and retrieve:  \n",
    "\n",
    "  - List of all reference names in format `components/{section}/{item}`.  \n",
    "  - Mapping of reference names to all locations where they are referenced by other references (nested reference). Locations are saved as strings in which each layer is separated by `/`.  \n",
    "  - Mapping of reference names to all other references they refer to in their definitions (nested references) - dependencies mapping.\n",
    "\n",
    "2. Check for \"clean\" references - references that do not include nested references in their definitions / references that do not have any dependencies.  \n",
    "\n",
    "3. Attach the dictionary definitions of the \"clean\" references to locations that they are referred to.  \n",
    "\n",
    "4. Update the dependencies mapping and \"clean\" references list. Repeat step 2, 3, and 4 until all references are \"clean\" or no more references can be \"cleaned\" (circular referencing).\n",
    "\n",
    "5. Return a flat mapping of global reference names (formatted `#/components/{section}/{item}`) to their expanded (resolved) definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bacbd46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"display: flex; justify-content: center; align-items: center; width: 100%; height: 100%;\">\n",
       "        <img src=\"https://mermaid.ink/img/CmZsb3djaGFydCBURAogICBPQ1tPQVMgQ29tcG9uZW50c10gLS0-fHRyYXZlcnNlfCBSTFtSZWZlcmVuY2UgbGlzdF0KICAgT0MgLS0-fHRyYXZlcnNlfCBOUkRbTmVzdGVkIHJlZmVyZW5jZSBkZXBlbmRlbmNpZXNdCiAgIE9DIC0tPnx0cmF2ZXJzZXwgTlJMW05lc3RlZCByZWZlcmVuY2UgbG9jYXRpb25zXQogICBOUkQgLS0-fGNoZWNrfCBDUltDbGVhbiByZWZlcmVuY2VzLCBpLmUuLCBubyBkZXBlbmRlbmNpZXNdCiAgIENSIC0tPnxjb21wYXJlfCBSTAogICBSTCAtLT58Q2xlYW4gPCBBbGx8IENSW0NsZWFuIHJlZmVyZW5jZXNdCiAgIHN1YmdyYXBoIFJlZmVyZW5jZSByZXNvbHV0aW9uCiAgIENSIC0tPnxhdHRhY2h8IE5STAogICBOUkwgLS0-fHVwZGF0ZXwgTlJECiAgIGVuZAogICBSTCAtLT58Q2xlYW4gPSBBbGx8IEVDW0ZsYXR0ZW5lZCBjb21wb25lbnRzXQogICBSTCAtLT58Q2xlYW4gdW5jaGFuZ2VkfCBFQwo=\" style=\"max-width: 100%; max-height: 100%; object-fit: contain;\" />\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mm(\"\"\"\n",
    "flowchart TD\n",
    "   OC[OAS Components] -->|traverse| RL[Reference list]\n",
    "   OC -->|traverse| NRD[Nested reference dependencies]\n",
    "   OC -->|traverse| NRL[Nested reference locations]\n",
    "   NRD -->|check| CR[Clean references, i.e., no dependencies]\n",
    "   CR -->|compare| RL\n",
    "   RL -->|Clean < All| CR[Clean references]\n",
    "   subgraph Reference resolution\n",
    "   CR -->|attach| NRL\n",
    "   NRL -->|update| NRD\n",
    "   end\n",
    "   RL -->|Clean = All| EC[Flattened components]\n",
    "   RL -->|Clean unchanged| EC\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5afbf46",
   "metadata": {},
   "source": [
    "**NOTE**: For reference locations, layers being separated by `/` may overlap with MIME types such as `text/plain` and `application/json`. Therefore, we need an utility function to scan for these MIME types and extract the correct layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e4637d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "MIME_TYPES = {\n",
    "    \"text\": [\n",
    "        \"plain\",\n",
    "        \"html\",\n",
    "        \"css\",\n",
    "        \"javascript\",\n",
    "        \"markdown\",\n",
    "        \"xml\",\n",
    "        \"csv\",\n",
    "        \"tab-separated-values\",\n",
    "        \"vcard\"\n",
    "    ],\n",
    "    \"application\": [\n",
    "        \"json\",\n",
    "        \"xml\",\n",
    "        \"javascript\",\n",
    "        \"x-www-form-urlencoded\",\n",
    "        \"multipart-form-data\",\n",
    "        \"pdf\",\n",
    "        \"zip\",\n",
    "        \"gzip\",\n",
    "        \"vnd.api+json\",\n",
    "        \"sql\",\n",
    "        \"octet-stream\",\n",
    "        \"ld+json\",\n",
    "        \"vnd.ms-excel\",\n",
    "        \"vnd.openxmlformats-officedocument.spreadsheetml.sheet\",\n",
    "        \"vnd.ms-word\",\n",
    "        \"vnd.openxmlformats-officedocument.wordprocessingml.document\",\n",
    "        \"vnd.ms-powerpoint\",\n",
    "        \"vnd.openxmlformats-officedocument.presentationml.presentation\",\n",
    "        \"x-tar\",\n",
    "        \"x-7z-compressed\",\n",
    "        \"vnd.android.package-archive\",\n",
    "        \"x-rar-compressed\",\n",
    "        \"x-bzip\",\n",
    "        \"x-bzip2\",\n",
    "        \"x-sh\",\n",
    "        \"x-java-archive\",\n",
    "        \"x-httpd-php\",\n",
    "        \"x-pkcs12\",\n",
    "        \"x-pkcs7-certificates\",\n",
    "        \"x-pkcs7-mime\",\n",
    "        \"x-pkcs7-signature\"\n",
    "    ],\n",
    "    \"image\": [\n",
    "        \"png\",\n",
    "        \"jpeg\",\n",
    "        \"gif\",\n",
    "        \"webp\",\n",
    "        \"svg+xml\",\n",
    "        \"bmp\",\n",
    "        \"tiff\",\n",
    "        \"x-icon\"\n",
    "    ],\n",
    "    \"audio\": [\n",
    "        \"mpeg\",\n",
    "        \"ogg\",\n",
    "        \"wav\",\n",
    "        \"webm\",\n",
    "        \"aac\",\n",
    "        \"midi\"\n",
    "    ],\n",
    "    \"video\": [\n",
    "        \"mp4\",\n",
    "        \"ogg\",\n",
    "        \"webm\",\n",
    "        \"x-msvideo\",\n",
    "        \"quicktime\"\n",
    "    ],\n",
    "    \"font\": [\n",
    "        \"ttf\",\n",
    "        \"otf\",\n",
    "        \"woff\",\n",
    "        \"woff2\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "def retrieve_ref_parts(refs: str):\n",
    "    \"\"\"Retrieve the parts of a reference string\"\"\"\n",
    "    # Split the reference string into raw parts and initialize the parts list\n",
    "    raw_parts = refs.split(\"/\")\n",
    "    parts = []\n",
    "\n",
    "    i = 0\n",
    "    while i < len(raw_parts) - 1:\n",
    "        # Extract consecutive parts\n",
    "        first_part = raw_parts[i]\n",
    "        second_part = raw_parts[i + 1]\n",
    "\n",
    "        # Check if the parts are valid MIME types\n",
    "        if first_part in MIME_TYPES and second_part in MIME_TYPES[first_part]:\n",
    "            # Combine the MIME types\n",
    "            parts.append(f\"{first_part}/{second_part}\")\n",
    "            i += 2\n",
    "        else:\n",
    "            parts.append(first_part)\n",
    "            i += 1\n",
    "\n",
    "    # Add the last part if it is not a MIME type\n",
    "    if raw_parts[-2] not in MIME_TYPES or raw_parts[-1] not in MIME_TYPES[raw_parts[-2]]:\n",
    "        parts.append(raw_parts[-1])\n",
    "\n",
    "    return parts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dc1bbc",
   "metadata": {},
   "source": [
    "Implementation of the described algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73699f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def extract_refs(\n",
    "    oas: dict  # The OpenAPI schema\n",
    ") -> dict:  # The extracted references (flattened)\n",
    "    refs = copy.deepcopy(oas)\n",
    "    refs_list = set()\n",
    "    refs_locations = {}\n",
    "    refs_dependencies = {}\n",
    "\n",
    "    # Traverse the components section of the spec\n",
    "    for section, items in refs[\"components\"].items():\n",
    "        for item in items:\n",
    "            refs_list.add(f\"components/{section}/{item}\")\n",
    "            refs_locations[f\"components/{section}/{item}\"] = []\n",
    "            refs_dependencies[f\"components/{section}/{item}\"] = set()\n",
    "    \n",
    "    # Initialize the clean_refs set\n",
    "    clean_refs = refs_list.copy()\n",
    "\n",
    "    # Traverse the spec and extract the references\n",
    "    def traverse_location(obj, path=\"\"):\n",
    "        for key, value in obj.items():\n",
    "            if key == \"$ref\":\n",
    "                # Determine the root of the reference and remove it from the clean_refs set\n",
    "                ref_root = \"/\".join(path.split(\"/\")[:3])\n",
    "                clean_refs.discard(ref_root)\n",
    "\n",
    "                # Extract the sub reference and add the current path to the list of locations\n",
    "                sub_ref = value[2:]\n",
    "                refs_locations[sub_ref].append(path)\n",
    "\n",
    "                # Add the sub reference to the dependencies of the current reference\n",
    "                refs_dependencies[ref_root].add(sub_ref)\n",
    "\n",
    "            elif isinstance(value, dict):\n",
    "                # Recursively traverse the object\n",
    "                traverse_location(value, f\"{path}/{key}\")\n",
    "\n",
    "    traverse_location(refs[\"components\"], \"components\")\n",
    "\n",
    "    # Attach the reference objects to the locations\n",
    "    def attach_clean_refs():\n",
    "        for ref in clean_refs:\n",
    "            # Extract the reference object\n",
    "            ref_obj = refs\n",
    "            ref_paths = retrieve_ref_parts(ref)\n",
    "            for part in ref_paths:\n",
    "                ref_obj = ref_obj[part]\n",
    "\n",
    "            # Extract the locations where the reference is used\n",
    "            locations = refs_locations[ref]\n",
    "\n",
    "            # Attach the reference object to the locations\n",
    "            for location in locations:\n",
    "                location_parts = retrieve_ref_parts(location)\n",
    "\n",
    "                obj = refs\n",
    "                prev = None\n",
    "                for part in location_parts:\n",
    "                    prev = obj\n",
    "                    obj = obj[part]\n",
    "\n",
    "                prev[location_parts[-1]] = ref_obj\n",
    "\n",
    "            # Remove the reference from the dependencies\n",
    "            for dependency in refs_dependencies:\n",
    "                refs_dependencies[dependency].discard(ref)\n",
    "\n",
    "    # Check if there are any clean references\n",
    "    def check_clean_refs():\n",
    "        clean_refs = set()\n",
    "        for ref, dependencies in refs_dependencies.items():\n",
    "            if len(dependencies) == 0:\n",
    "                clean_refs.add(ref)\n",
    "        return clean_refs\n",
    "    \n",
    "    # Iterate until all references are attached or no progress is made\n",
    "    prev_nof_clean = None\n",
    "    while len(clean_refs) < len(refs_list) and prev_nof_clean != len(clean_refs):\n",
    "        prev_nof_clean = len(clean_refs)\n",
    "        attach_clean_refs()\n",
    "        clean_refs = check_clean_refs()\n",
    "\n",
    "    # Flatten the references\n",
    "    flatten_refs = {}\n",
    "    for section, items in refs[\"components\"].items():\n",
    "        for item in items:\n",
    "            flatten_refs[f\"#/components/{section}/{item}\"] = refs[\"components\"][section][item]\n",
    "\n",
    "    return flatten_refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855b9318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/ninjalabo/llmcam/blob/main/llmcam/core/oas_to_requests.py#L123){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### extract_refs\n",
       "\n",
       ">      extract_refs (oas:dict)\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| oas | dict | The OpenAPI schema |\n",
       "| **Returns** | **dict** | **The extracted references (flattened)** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/ninjalabo/llmcam/blob/main/llmcam/core/oas_to_requests.py#L123){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### extract_refs\n",
       "\n",
       ">      extract_refs (oas:dict)\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| oas | dict | The OpenAPI schema |\n",
       "| **Returns** | **dict** | **The extracted references (flattened)** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(extract_refs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e0d8cb",
   "metadata": {},
   "source": [
    "We can demonstrate an example usage with Road DigiTraffic. The API server includes a reference called `Address` which involves nested referencing and circular referencing. The algorithm successfully expands possible parts of this reference and ignores the circular referencing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d22c04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'object',\n",
       " 'properties': {'postcode': {'type': 'string'},\n",
       "  'city': {'required': ['values'],\n",
       "   'type': 'object',\n",
       "   'properties': {'values': {'required': ['values'],\n",
       "     'type': 'object',\n",
       "     'properties': {'values': {'type': 'array',\n",
       "       'xml': {'name': 'value',\n",
       "        'namespace': 'http://datex2.eu/schema/3/common'},\n",
       "       'items': {'type': 'object',\n",
       "        'properties': {'value': {'type': 'string'},\n",
       "         'lang': {'type': 'string', 'xml': {'attribute': True}}}}}}}}},\n",
       "  'countryCode': {'type': 'string'},\n",
       "  'addressLines': {'type': 'array',\n",
       "   'xml': {'name': 'addressLine'},\n",
       "   'items': {'$ref': '#/components/schemas/AddressLine'}},\n",
       "  'get_AddressExtension': {'$ref': '#/components/schemas/_ExtensionType'}}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refs = extract_refs(oas)\n",
    "refs[\"#/components/schemas/Address\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02022cd",
   "metadata": {},
   "source": [
    "## OAS schema to GPT-compatible schema\n",
    "\n",
    "GPT currently recognizes only a limited number of descriptors when defining tools schema. Some of these descriptors (fields) can be directly transferred from OAS schema to tools, but many existing OAS schema fields will not be recognized by GPT and can cause errors. Therefore, transformation from OAS schemas to GPT-compatible schemas is necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff74bc7e",
   "metadata": {},
   "source": [
    "GPT currently recognizes these fields:\n",
    "\n",
    "1. `type`  \n",
    "Specifies the data type of the value. Common types include:  \n",
    "  - `string` – A text string.  \n",
    "  - `number` – A numeric value (can be integer or floating point).  \n",
    "  - `integer` – A whole number.  \n",
    "  - `boolean` – A true/false value.  \n",
    "  - `array` – A list of items (you can define the type of items in the array as well).  \n",
    "  - `object` – A JSON object (with properties, which can be further defined with their own types).  \n",
    "  - `null` – A special type to represent a null or absent value.  \n",
    "  - `any` – Allows any type, typically used for flexible inputs or outputs.  \n",
    "2. `default`: Provides a default value for the field if the user doesn't supply one. It can be any valid type based on the expected schema.  \n",
    "3. `enum`: Specifies a list of acceptable values for a field. It restricts the input to one of the predefined values in the array.  \n",
    "4. `properties`: Used for objects, this defines the subfields of an object and their respective types.  \n",
    "5. `items`: Defines the type of items in an array. For example, you can specify that an array contains only strings or integers.  \n",
    "6. `minLength`, `maxLength`: Specifies minimum and maximum lengths for `string` parameters.  \n",
    "7. `minItems`, `maxItems`: Specifies mininum and maximum number of items for `array` parameters.  \n",
    "8. `pattern`: Specifies a regular expression that the string must match for `string` parameters.  \n",
    "9. `required`: A list of required fields for an `object`. Specifies that certain fields within an `object` must be provided.  \n",
    "10. `additionalProperties`: Specifies whether additional properties are allowed in an `object`. If set to `false`, no properties outside of those defined in properties will be accepted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f62ab0",
   "metadata": {},
   "source": [
    "As such, we can extract corresponding fields from OAS schema, and converts all additional fields into parameter description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656e3573",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Directly transferable properties from OAS to GPT-compatible schema\n",
    "TRANSFERABLE_TYPES = [\n",
    "    \"type\", \"description\", \"default\", \"enum\", \"pattern\", \"additionalProperties\",\n",
    "    \"minLength\", \"maxLength\", \"minItems\", \"maxItems\"\n",
    "]\n",
    "\n",
    "# Function to transform OAS schema to GPT-compatible schema\n",
    "def transform_property(\n",
    "    prop: dict,  # The property to transform\n",
    "    flatten_refs: dict = {}  # The flattened references\n",
    ") -> tuple[dict, bool]:  # The transformed property and whether it is a required property\n",
    "\n",
    "    # If the property is a schema, flatten it\n",
    "    if \"schema\" in prop:\n",
    "        prop = copy.deepcopy(prop)\n",
    "        prop.update(prop[\"schema\"])\n",
    "        prop.pop(\"schema\")\n",
    "\n",
    "    # Extract the required field\n",
    "    required = prop.get(\"required\", False)\n",
    "    \n",
    "    # If the property is a reference, return it as is\n",
    "    if \"$ref\" in prop:\n",
    "        if prop[\"$ref\"] in flatten_refs:\n",
    "            ref_prop, _ = transform_property(flatten_refs[prop[\"$ref\"]], flatten_refs)\n",
    "            return ref_prop, required\n",
    "        else:\n",
    "            # If the reference is not found, return the reference as is\n",
    "            return prop, required \n",
    "    \n",
    "    # If the property is an object, transform it\n",
    "    new_prop = {}\n",
    "    additionals = {}\n",
    "\n",
    "    # If required is a list, it is directly transferable to GPT-compatible schema\n",
    "    if isinstance(required, list): \n",
    "        new_prop[\"required\"] = required\n",
    "        required = True\n",
    "\n",
    "    for key, value in prop.items():\n",
    "        if key in TRANSFERABLE_TYPES:\n",
    "            new_prop[key] = value\n",
    "        elif key == \"items\":\n",
    "            # Handle array items recursively\n",
    "            item_prop, _ = transform_property(value, flatten_refs)\n",
    "            new_prop[key] = item_prop\n",
    "        elif key == \"properties\":\n",
    "            # Handle nested properties recursively\n",
    "            new_prop[key] = {}\n",
    "            new_prop[\"required\"] = [] if \"required\" not in new_prop else new_prop[\"required\"]\n",
    "            for sub_key, sub_value in value.items():\n",
    "                sub_prop, sub_required = transform_property(sub_value, flatten_refs)\n",
    "                new_prop[key][sub_key] = sub_prop\n",
    "                if sub_required:\n",
    "                    new_prop[\"required\"].append(sub_key)\n",
    "        elif key == \"required\":\n",
    "            # Skip required field since it is handled in the properties section\n",
    "            continue\n",
    "        else:\n",
    "            # Collect unrecognized fields in additionals dictionary\n",
    "            additionals[key] = value\n",
    "\n",
    "    # Add the additionals dictionary if it is not empty\n",
    "    if len(additionals) > 0:\n",
    "        additional_info = \"; \".join([f\"{k.capitalize()}: {v}\" for k, v in additionals.items()])\n",
    "        if \"description\" in new_prop:\n",
    "            new_prop[\"description\"] += f\" ({additional_info})\"\n",
    "        else:\n",
    "            new_prop[\"description\"] = f\"({additional_info})\"\n",
    "\n",
    "    # Remove None values and return the transformed property\n",
    "    return {k: v for k, v in new_prop.items() if v is not None}, required\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6618d4c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/ninjalabo/llmcam/blob/main/llmcam/core/oas_to_requests.py#L221){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### transform_property\n",
       "\n",
       ">      transform_property (prop:dict, flatten_refs:dict={})\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| prop | dict |  | The property to transform |\n",
       "| flatten_refs | dict | {} | The flattened references |\n",
       "| **Returns** | **tuple** |  | **The transformed property and whether it is a required property** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/ninjalabo/llmcam/blob/main/llmcam/core/oas_to_requests.py#L221){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### transform_property\n",
       "\n",
       ">      transform_property (prop:dict, flatten_refs:dict={})\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| prop | dict |  | The property to transform |\n",
       "| flatten_refs | dict | {} | The flattened references |\n",
       "| **Returns** | **tuple** |  | **The transformed property and whether it is a required property** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(transform_property)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418ecd9a",
   "metadata": {},
   "source": [
    "Test usage with complex parameters from DigiTraffic endpoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c6f546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'description': 'If parameter is given result will only contain update status. (Name: lastUpdated; In: query)', 'type': 'boolean', 'default': False}\n",
      "Required: False\n",
      "\n",
      "\n",
      "{'description': 'Road number (Name: roadNumber; In: query; Format: int32)', 'type': 'integer'}\n",
      "Required: False\n",
      "\n",
      "\n",
      "{'description': 'Minimum x coordinate (longitude) Coordinates are in WGS84 format in decimal degrees. Values between 19.0 and 32.0. (Name: xMin; In: query; Maximum: 32; Exclusivemaximum: False; Minimum: 19; Exclusiveminimum: False; Format: double)', 'type': 'number', 'default': 19}\n",
      "Required: False\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parameters = [\n",
    "    {\n",
    "        \"name\": \"lastUpdated\",\n",
    "        \"in\": \"query\",\n",
    "        \"description\": \"If parameter is given result will only contain update status.\",\n",
    "        \"required\": False,\n",
    "            \"schema\": {\n",
    "                \"type\": \"boolean\",\n",
    "                \"default\": False\n",
    "            }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"roadNumber\",\n",
    "        \"in\": \"query\",\n",
    "        \"description\": \"Road number\",\n",
    "        \"required\": False,\n",
    "        \"schema\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"format\": \"int32\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"xMin\",\n",
    "        \"in\": \"query\",\n",
    "        \"description\": \"Minimum x coordinate (longitude) Coordinates are in WGS84 format in decimal degrees. Values between 19.0 and 32.0.\",\n",
    "        \"required\": False,\n",
    "        \"schema\": {\n",
    "            \"maximum\": 32,\n",
    "            \"exclusiveMaximum\": False,\n",
    "            \"minimum\": 19,\n",
    "            \"exclusiveMinimum\": False,\n",
    "            \"type\": \"number\",\n",
    "            \"format\": \"double\",\n",
    "            \"default\": 19\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "for param in parameters:\n",
    "    param, required = transform_property(param)\n",
    "    print(param)\n",
    "    print(f\"Required: {required}\\n\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71473afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Sample OAS schema with nested schema properties\n",
    "flatten_refs = {\n",
    "    \"#/components/schemas/dimensions\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"width\": {\n",
    "                \"type\": \"number\",\n",
    "                \"minimum\": 0,\n",
    "                \"description\": \"Width in centimeters.\",\n",
    "                \"required\": True\n",
    "            },\n",
    "            \"height\": {\n",
    "                \"type\": \"number\",\n",
    "                \"minimum\": 0,\n",
    "                \"description\": \"Height in centimeters.\",\n",
    "                \"required\": True\n",
    "            },\n",
    "            \"depth\": {\n",
    "                \"type\": \"number\",\n",
    "                \"minimum\": 0,\n",
    "                \"description\": \"Depth in centimeters.\",\n",
    "                \"required\": True\n",
    "            }\n",
    "        },\n",
    "        \"description\": \"Dimensions of the product.\"\n",
    "    }\n",
    "}\n",
    "        \n",
    "nested_param = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"product\": {\n",
    "            \"type\": \"object\",\n",
    "            \"schema\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"id\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Unique identifier for the product.\"\n",
    "                    },\n",
    "                    \"details\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"schema\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                                \"weight\": {\n",
    "                                    \"type\": \"number\",\n",
    "                                    \"minimum\": 0,\n",
    "                                    \"description\": \"Weight of the product in kilograms.\"\n",
    "                                },\n",
    "                                \"dimensions\": {\n",
    "                                    \"schema\":\n",
    "                                        {\n",
    "                                        \"$ref\": \"#/components/schemas/dimensions\"\n",
    "                                        }\n",
    "                                }\n",
    "                            }\n",
    "                        },\n",
    "                        \"description\": \"Detailed specifications of the product.\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"description\": \"Product information.\"\n",
    "        },\n",
    "        \"category\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Category of the product.\"\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"product\", \"category\"]\n",
    "}\n",
    "\n",
    "transformed_param, _ = transform_property(nested_param, flatten_refs)\n",
    "assert transformed_param == {\n",
    "  \"required\": [\n",
    "    \"product\",\n",
    "    \"category\"\n",
    "  ],\n",
    "  \"type\": \"object\",\n",
    "  \"properties\": {\n",
    "    \"product\": {\n",
    "      \"type\": \"object\",\n",
    "      \"description\": \"Product information.\",\n",
    "      \"properties\": {\n",
    "        \"id\": {\n",
    "          \"type\": \"string\",\n",
    "          \"description\": \"Unique identifier for the product.\"\n",
    "        },\n",
    "        \"details\": {\n",
    "          \"type\": \"object\",\n",
    "          \"description\": \"Detailed specifications of the product.\",\n",
    "          \"properties\": {\n",
    "            \"weight\": {\n",
    "              \"type\": \"number\",\n",
    "              \"description\": \"Weight of the product in kilograms. (Minimum: 0)\"\n",
    "            },\n",
    "            \"dimensions\": {\n",
    "              \"type\": \"object\",\n",
    "              \"description\": \"Dimensions of the product.\",\n",
    "              \"properties\": {\n",
    "                \"width\": {\n",
    "                  \"type\": \"number\",\n",
    "                  \"description\": \"Width in centimeters. (Minimum: 0)\"\n",
    "                },\n",
    "                \"height\": {\n",
    "                  \"type\": \"number\",\n",
    "                  \"description\": \"Height in centimeters. (Minimum: 0)\"\n",
    "                },\n",
    "                \"depth\": {\n",
    "                  \"type\": \"number\",\n",
    "                  \"description\": \"Depth in centimeters. (Minimum: 0)\"\n",
    "                }\n",
    "              },\n",
    "              \"required\": [\n",
    "                \"width\",\n",
    "                \"height\",\n",
    "                \"depth\"\n",
    "              ]\n",
    "            }\n",
    "          },\n",
    "          \"required\": []\n",
    "        }\n",
    "      },\n",
    "      \"required\": []\n",
    "    },\n",
    "    \"category\": {\n",
    "      \"type\": \"string\",\n",
    "      \"description\": \"Category of the product.\"\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c309cc",
   "metadata": {},
   "source": [
    "## OAS to schema\n",
    "\n",
    "We can combine the above utilities to extract important information about the functions and creates a GPT-compatible tools schema. The idea is to convert all necessary information for generating an API request to a parameter for GPT to provide. As such, the parameters of each function in this tools schema will include:\n",
    "\n",
    "- `path`: dictionary for path parameters that maps parameter names to schema\n",
    "- `query`: dictionary for query parameters that maps parameter names to schema\n",
    "- `body`: request body schema\n",
    "\n",
    "Apart from the parameters that GPT should provide, we also need constant values for each function (endpoint). These values should be saved as `metadata` in tools schema:\n",
    "\n",
    "- `url`: URL to send requests to  \n",
    "- `method`: HTTP method for each endpoint  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785e5e28",
   "metadata": {},
   "source": [
    "### Auxiliary fixup function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31797538",
   "metadata": {},
   "source": [
    "To avoid writing Python functions for each of these endpoints, we can use a universal fixup (wrapper) function to execute API requests based on the above data.  \n",
    "\n",
    "**NOTE**: Apart from the above inputs, implementation of this fixup function will be more simple with an extra `metadata` for acceptable query parameters. This is because GPT often flattens parameter inputs for simple requests, making it difficult to differentiate between `path` and `query` parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416fea7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def generate_request(\n",
    "    function_name: str,  # The name of the function\n",
    "    url: str,  # The URL of the request\n",
    "    method: str,  # The method of the request\n",
    "    path: dict = {},  # The path parameters of the request\n",
    "    query: dict = {},  # The query parameters of the request\n",
    "    body: dict = {},  # The body of the request\n",
    "    accepted_queries: list = [],  # The accepted queries of the request\n",
    "    **kwargs  # Additional parameters\n",
    ") -> dict:  # The response of the request\n",
    "    \"\"\"Generate a request from the function name and parameters.\"\"\"\n",
    "    # Extract the URL and method from the tools if not provided\n",
    "    if url is None or method is None:\n",
    "        raise ValueError(\"URL and method must be provided.\")\n",
    "    \n",
    "    # Prepare the request\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    # Extract the accepted queries\n",
    "    queries = {k: v for k, v in query.items() if k in accepted_queries}\n",
    "    queries.update({k: v for k, v in kwargs.items() if k in accepted_queries})\n",
    "\n",
    "    # Execute the request\n",
    "    response = requests.request(\n",
    "        method,\n",
    "        url.format(**path, **kwargs),\n",
    "        headers=headers,\n",
    "        params=queries if len(queries) > 0 else None,\n",
    "        json=body if len(body) > 0 else None\n",
    "    )\n",
    "\n",
    "    # Return the response (either as JSON or text)\n",
    "    try:\n",
    "        return response.json()\n",
    "    except:\n",
    "        return {\"message\": response.text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57934241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/ninjalabo/llmcam/blob/main/llmcam/core/oas_to_requests.py#L289){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### generate_request\n",
       "\n",
       ">      generate_request (function_name:str, url:str, method:str, path:dict={},\n",
       ">                        query:dict={}, body:dict={}, accepted_queries:list=[],\n",
       ">                        **kwargs)\n",
       "\n",
       "*Generate a request from the function name and parameters.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| function_name | str |  | The name of the function |\n",
       "| url | str |  | The URL of the request |\n",
       "| method | str |  | The method of the request |\n",
       "| path | dict | {} | The path parameters of the request |\n",
       "| query | dict | {} | The query parameters of the request |\n",
       "| body | dict | {} | The body of the request |\n",
       "| accepted_queries | list | [] | The accepted queries of the request |\n",
       "| kwargs |  |  |  |\n",
       "| **Returns** | **dict** |  | **The response of the request** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/ninjalabo/llmcam/blob/main/llmcam/core/oas_to_requests.py#L289){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### generate_request\n",
       "\n",
       ">      generate_request (function_name:str, url:str, method:str, path:dict={},\n",
       ">                        query:dict={}, body:dict={}, accepted_queries:list=[],\n",
       ">                        **kwargs)\n",
       "\n",
       "*Generate a request from the function name and parameters.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| function_name | str |  | The name of the function |\n",
       "| url | str |  | The URL of the request |\n",
       "| method | str |  | The method of the request |\n",
       "| path | dict | {} | The path parameters of the request |\n",
       "| query | dict | {} | The query parameters of the request |\n",
       "| body | dict | {} | The body of the request |\n",
       "| accepted_queries | list | [] | The accepted queries of the request |\n",
       "| kwargs |  |  |  |\n",
       "| **Returns** | **dict** |  | **The response of the request** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(generate_request)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450c68eb",
   "metadata": {},
   "source": [
    "### API Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41993212",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def api_schema(\n",
    "    base_url: str,  # The base URL of the API\n",
    "    oas: dict,  # The OpenAPI schema\n",
    "    service_name: Optional[str] = None,  # The name of the service\n",
    "    fixup: Optional[Callable] = None, # Fixup function to execute a REST API request\n",
    ") -> dict:  # The api schema\n",
    "    \"\"\"Form the tools schema from the OpenAPI schema.\"\"\"\n",
    "    \n",
    "    # Extract the references\n",
    "    flatten_refs = extract_refs(oas)\n",
    "    \n",
    "    # Initialize the tools list\n",
    "    tools = []\n",
    "\n",
    "    # Traverse the paths section of the spec\n",
    "    for path, methods in oas[\"paths\"].items():\n",
    "        for method, info in methods.items():\n",
    "            # Extract the function name\n",
    "            name = info[\"operationId\"] if \"operationId\" in info else \\\n",
    "                f\"{method}{path.replace('/', '_').replace('{', 'by').replace('}', '').replace('-', '_')}\"\n",
    "            name = re.sub(r'[^a-zA-Z0-9_-]', '_', name)\n",
    "\n",
    "            # Extract the function description\n",
    "            description = info[\"description\"] if \"description\" in info else info.get(\"summary\", \"\")\n",
    "\n",
    "            # Extract the function parameters\n",
    "            parameters = {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {},   \n",
    "                \"required\": []\n",
    "            }\n",
    "            accepted_queries = []\n",
    "\n",
    "            # Extract endpoint parameters\n",
    "            if \"parameters\" in info:\n",
    "                for param in info[\"parameters\"]:\n",
    "                    # Extract the parameter location (query, path, header, cookie)\n",
    "                    location = param.get(\"in\", \"query\")\n",
    "\n",
    "                    # Initialize the parameter object based on location\n",
    "                    if location not in parameters[\"properties\"]:\n",
    "                        parameters[\"properties\"][location] = {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {},\n",
    "                            \"required\": []\n",
    "                        }\n",
    "\n",
    "                    # Extract the parameter schema\n",
    "                    param_obj, required = transform_property(param, flatten_refs)\n",
    "\n",
    "                    # Add the parameter to the tools\n",
    "                    parameters[\"properties\"][location][\"properties\"][param[\"name\"]] = param_obj\n",
    "                    if required or location == \"path\":\n",
    "                        parameters[\"properties\"][location][\"required\"].append(param[\"name\"])\n",
    "                        parameters[\"required\"].append(location)\n",
    "\n",
    "                    # Add the parameter to the accepted queries\n",
    "                    if location == \"query\":\n",
    "                        accepted_queries.append(param[\"name\"])\n",
    "                    \n",
    "            # Extract the function body\n",
    "            body = {}\n",
    "            if \"requestBody\" in info and \"content\" in info[\"requestBody\"] \\\n",
    "                    and \"application/json\" in info[\"requestBody\"][\"content\"] \\\n",
    "                    and \"schema\" in info[\"requestBody\"][\"content\"][\"application/json\"]:\n",
    "                body = info[\"requestBody\"][\"content\"][\"application/json\"]\n",
    "                body, _ = transform_property(body, flatten_refs)\n",
    "                parameters[\"properties\"][\"body\"] = body\n",
    "                parameters[\"required\"].append(\"body\")\n",
    "\n",
    "            # Remove duplicate required properties\n",
    "            parameters[\"required\"] = list(set(parameters[\"required\"]))\n",
    "                \n",
    "            # Conclude the function information\n",
    "            function = {\n",
    "                \"name\": name,\n",
    "                \"description\": description,\n",
    "                \"parameters\": parameters,\n",
    "                \"metadata\": {\n",
    "                    \"url\": base_url + path,\n",
    "                    \"method\": method,\n",
    "                    \"accepted_queries\": accepted_queries,\n",
    "                }\n",
    "            }\n",
    "            if fixup: function['fixup'] = f\"{fixup.__module__}.{fixup.__name__}\"\n",
    "            if service_name: function[\"metadata\"][\"service\"] = service_name\n",
    "\n",
    "            # Add the function to the tools\n",
    "            tools.append(\n",
    "                {\n",
    "                    \"type\": \"function\",\n",
    "                    \"function\": function\n",
    "                }\n",
    "            )\n",
    "        \n",
    "    return tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e1dc49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/ninjalabo/llmcam/blob/main/llmcam/core/oas_to_requests.py#L329){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### api_schema\n",
       "\n",
       ">      api_schema (base_url:str, oas:dict, service_name:Optional[str]=None,\n",
       ">                  fixup:Optional[Callable]=None)\n",
       "\n",
       "*Form the tools schema from the OpenAPI schema.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| base_url | str |  | The base URL of the API |\n",
       "| oas | dict |  | The OpenAPI schema |\n",
       "| service_name | Optional | None | The name of the service |\n",
       "| fixup | Optional | None | Fixup function to execute a REST API request |\n",
       "| **Returns** | **dict** |  | **The api schema** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/ninjalabo/llmcam/blob/main/llmcam/core/oas_to_requests.py#L329){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### api_schema\n",
       "\n",
       ">      api_schema (base_url:str, oas:dict, service_name:Optional[str]=None,\n",
       ">                  fixup:Optional[Callable]=None)\n",
       "\n",
       "*Form the tools schema from the OpenAPI schema.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| base_url | str |  | The base URL of the API |\n",
       "| oas | dict |  | The OpenAPI schema |\n",
       "| service_name | Optional | None | The name of the service |\n",
       "| fixup | Optional | None | Fixup function to execute a REST API request |\n",
       "| **Returns** | **dict** |  | **The api schema** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(api_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8937d79f",
   "metadata": {},
   "source": [
    "Example with Road DigiTraffic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afbf15b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'function',\n",
       " 'function': {'name': 'weatherDataHistory',\n",
       "  'description': 'List the history of sensor values from the weather road station',\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'path': {'type': 'object',\n",
       "     'properties': {'stationId': {'description': 'Weather station id (Name: stationId; In: path; Format: int64)',\n",
       "       'type': 'integer'}},\n",
       "     'required': ['stationId']},\n",
       "    'query': {'type': 'object',\n",
       "     'properties': {'from': {'description': 'Fetch history after given date time (Name: from; In: query; Format: date-time)',\n",
       "       'type': 'string'},\n",
       "      'to': {'description': 'Limit history to given date time (Name: to; In: query; Format: date-time)',\n",
       "       'type': 'string'}},\n",
       "     'required': []}},\n",
       "   'required': ['path']},\n",
       "  'metadata': {'url': 'https://tie.digitraffic.fi/api/beta/weather-history-data/{stationId}',\n",
       "   'method': 'get',\n",
       "   'accepted_queries': ['from', 'to'],\n",
       "   'service': 'Traffic Information'},\n",
       "  'fixup': '__main__.generate_request'}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools = api_schema(\n",
    "    base_url=\"https://tie.digitraffic.fi\",\n",
    "    oas=oas,\n",
    "    service_name=\"Traffic Information\",\n",
    "    fixup=generate_request\n",
    ")\n",
    "tools[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1490c344",
   "metadata": {},
   "source": [
    "## Simulated GPT workflow\n",
    "\n",
    "Test integrating with our current GPT framework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efda34fa-0ca6-4e89-a248-4e7e24518f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31m>> \u001b[43m\u001b[31mSystem:\u001b[0m\n",
      "You are a helpful system administrator. Use the supplied tools to help the user.\n",
      "\u001b[1m\u001b[31m>> \u001b[43m\u001b[32mUser:\u001b[0m\n",
      "Get the weather camera information for the stations with ID C01503 and C01504.\n",
      "\u001b[1m\u001b[31m>> \u001b[43m\u001b[34mAssistant:\u001b[0m\n",
      "Here is the weather camera information for the stations with ID C01503 and C01504:  ### Station\n",
      "C01503 - **Name**: Road 51 Inkoo (Tie 51 Inkoo in Finnish) - **Location**: Municipality of Inkoo in\n",
      "Uusimaa province - **Coordinates**: Latitude 60.05374, Longitude 23.99616 - **Camera Type**: BOSCH -\n",
      "**Nearest Weather Station ID**: 1013 - **Collection Status**: Gathering - **Updated Time**:\n",
      "2024-12-10T15:25:40Z - **Collection Interval**: Every 600 seconds - **Preset Images**:   -\n",
      "Inkooseen: [![Image](https://weathercam.digitraffic.fi/C0150301.jpg)](https://weathercam.digitraffic\n",
      ".fi/C0150301.jpg)   - Hankoon: [![Image](https://weathercam.digitraffic.fi/C0150302.jpg)](https://we\n",
      "athercam.digitraffic.fi/C0150302.jpg)   - Tienpinta: [![Image](https://weathercam.digitraffic.fi/C01\n",
      "50309.jpg)](https://weathercam.digitraffic.fi/C0150309.jpg)  ### Station C01504 - **Name**: Road 2\n",
      "Karkkila, Kappeli (Tie 2 Karkkila, Kappeli in Finnish) - **Location**: Municipality of Karkkila in\n",
      "Uusimaa province - **Coordinates**: Latitude 60.536727, Longitude 24.235601 - **Camera Type**:\n",
      "HIKVISION - **Nearest Weather Station ID**: 1052 - **Collection Status**: Gathering - **Updated\n",
      "Time**: 2024-12-10T15:27:01Z - **Collection Interval**: Every 600 seconds - **Preset Images**:   -\n",
      "Poriin: [![Image](https://weathercam.digitraffic.fi/C0150401.jpg)](https://weathercam.digitraffic.fi\n",
      "/C0150401.jpg)   - Helsinkiin: [![Image](https://weathercam.digitraffic.fi/C0150402.jpg)](https://we\n",
      "athercam.digitraffic.fi/C0150402.jpg)   - Tienpinta: [![Image](https://weathercam.digitraffic.fi/C01\n",
      "50409.jpg)](https://weathercam.digitraffic.fi/C0150409.jpg)  If you have any more questions or need\n",
      "further assistance, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "from llmcam.core.fc import *\n",
    "\n",
    "messages = form_msgs([\n",
    "    (\"system\", \"You are a helpful system administrator. Use the supplied tools to help the user.\"),\n",
    "    (\"user\", \"Get the weather camera information for the stations with ID C01503 and C01504.\"),\n",
    "])\n",
    "complete(messages, tools=tools)\n",
    "print_msgs(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c809f4d1-b0c1-4528-bcf8-682ca49b26f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
