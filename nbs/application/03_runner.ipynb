{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runner\n",
    "\n",
    "> Python module for running Chat UI Application from a general Python script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the main app, an utility function is implemented to run the app just by importing and executing this function to a Python file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp application.runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import asyncio\n",
    "import os\n",
    "import uvicorn\n",
    "from fastcore.parallel import startthread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can implement the runner based on actual implementation of `JuViApp`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def run_llmcam(\n",
    "    host=\"0.0.0.0\",  # The host to listen on\n",
    "    port=5001,  # The port to listen on\n",
    "    data_path=None,  # The path to the data directory\n",
    "    openai_key=None,  # The OpenAI API key\n",
    "):\n",
    "    \"\"\"Run the LLMCAM chatbot application\"\"\"\n",
    "    # Import app from chat_ui base module\n",
    "    from llmcam.application.chat_ui import app\n",
    "\n",
    "    # Set the data path and OpenAI key\n",
    "    if data_path is not None: os.environ[\"LLMCAM_DATA\"] = data_path\n",
    "    if openai_key is not None: os.environ[\"OPENAI_API_KEY\"] = openai_key\n",
    "    \n",
    "    # Run the server with uvicorn\n",
    "    server = uvicorn.Server(uvicorn.Config(app, host=host, port=port))\n",
    "    async def async_run_server(server): await server.serve()\n",
    "    asyncio.run(async_run_server(server))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### run_llmcam\n",
       "\n",
       ">      run_llmcam (host='0.0.0.0', port=5001, data_path=None, openai_key=None)\n",
       "\n",
       "*Run the LLMCAM chatbot application*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| host | str | 0.0.0.0 | The host to listen on |\n",
       "| port | int | 5001 | The port to listen on |\n",
       "| data_path | NoneType | None | The path to the data directory |\n",
       "| openai_key | NoneType | None | The OpenAI API key |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### run_llmcam\n",
       "\n",
       ">      run_llmcam (host='0.0.0.0', port=5001, data_path=None, openai_key=None)\n",
       "\n",
       "*Run the LLMCAM chatbot application*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| host | str | 0.0.0.0 | The host to listen on |\n",
       "| port | int | 5001 | The port to listen on |\n",
       "| data_path | NoneType | None | The path to the data directory |\n",
       "| openai_key | NoneType | None | The OpenAI API key |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(run_llmcam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
