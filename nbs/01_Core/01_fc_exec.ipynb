{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Calling\n",
    "\n",
    "> Python module to execute function calling from GPT messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core.fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT responses - Schema to Tool calls\n",
    "\n",
    "By default, OpenAI GPT accounts for its 'tools' and understands them via an inputted tools schema. Based on the description in the provided schema, GPT selects appropriate tools to answer to the user's messages. However, GPT does not execute these tools directly, but only provides a response message that contains the name and arguments for the selected tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example using GPT with a tool for getting weather information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to get weather information\n",
    "from typing import Optional\n",
    "\n",
    "def get_weather_information(\n",
    "    city: str,\n",
    "    zip_code: Optional[str] = None,\n",
    "):\n",
    "    return {\n",
    "        \"city\": city,\n",
    "        \"zip_code\": zip_code,\n",
    "        \"temparature\": 25,\n",
    "        \"humidity\": 80,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tools schema with 'get_weather_information' function\n",
    "tools = [{\n",
    "    'type': 'function',\n",
    "    'function': {\n",
    "        'name': 'get_weather_information',\n",
    "        'description': 'Get weather information for a given location',\n",
    "        'parameters': {\n",
    "            'type': 'object',\n",
    "            'properties': {\n",
    "                'city': {\n",
    "                    'type': 'string',\n",
    "                    'description': 'City name'\n",
    "                },\n",
    "                'zip_code': {\n",
    "                    'anyOf': [\n",
    "                        {'type': 'string'},\n",
    "                        {'type': 'null'},\n",
    "                    ],\n",
    "                },\n",
    "            },\n",
    "            'required': ['city'],\n",
    "        },\n",
    "    }\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': None,\n",
      " 'refusal': None,\n",
      " 'role': 'assistant',\n",
      " 'tool_calls': [{'function': {'arguments': '{\"city\":\"New York\"}',\n",
      "                              'name': 'get_weather_information'},\n",
      "                 'id': 'call_OM0VepmBDaPN6TbUd4P9lXur',\n",
      "                 'type': 'function'}]}\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "# Generate the response using the chat API\n",
    "import openai\n",
    "from pprint import pprint\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': 'You can get weather information for a given location using the `get_weather_information` function',\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'What is the weather in New York?',\n",
    "    }\n",
    "]\n",
    "\n",
    "response = openai.chat.completions.create(model=\"gpt-4o\", messages=messages, tools=tools)\n",
    "pprint(response.choices[0].message.to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example illustrates the raw response from GPT for a tool call and related-information that can be retrieved from it. Essentially, we can extract:\n",
    "\n",
    "- **Function name**: alpha-numeric string unique for each tool in tools schema  \n",
    "- **Arguments**: Inputs to feed into the function presented in key-value pairs  \n",
    "\n",
    "Apart from function-specific information, we also obtain an ID for this tool call. This is necessary to match this tool call to its results and generate further messages, as demonstrated in the following parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('get_weather_information',\n",
       " '{\"city\":\"New York\"}',\n",
       " 'call_OM0VepmBDaPN6TbUd4P9lXur')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "# Get the function name and parameters from the response\n",
    "tool_calls = response.choices[0].message.to_dict()['tool_calls']\n",
    "function_name = tool_calls[0]['function']['name']\n",
    "function_parameters = tool_calls[0]['function']['arguments']\n",
    "tool_call_id = tool_calls[0]['id']\n",
    "\n",
    "function_name, function_parameters, tool_call_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'city': 'New York', 'humidity': 80, 'temparature': 25, 'zip_code': None}\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "# Call the function with the parameters\n",
    "import json\n",
    "\n",
    "function = globals()[function_name]\n",
    "results = function(**json.loads(function_parameters))\n",
    "pprint(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can continue the previous conversation by adding both the tool call initiated by GPT and the results of such tool call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "messages.append(response.choices[0].message.to_dict())\n",
    "messages.append({\n",
    "    'role': 'tool',\n",
    "    'content': json.dumps({\n",
    "        **json.loads(function_parameters),\n",
    "        function_name: results,\n",
    "    }),\n",
    "    'tool_call_id': tool_call_id,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': 'The current weather in New York is 25Â°C with a humidity level of '\n",
      "            '80%.',\n",
      " 'refusal': None,\n",
      " 'role': 'assistant'}\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "response = openai.chat.completions.create(model=\"gpt-4o\", messages=messages, tools=tools)\n",
    "pprint(response.choices[0].message.to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This overall workflow should be the guideline for us to implement the function to execute function calling by following its steps:\n",
    "\n",
    "1. Generate tool call(s) with GPT response\n",
    "2. Add tool call to conversation (`messages`)\n",
    "3. Execute tool call in system with the given name and arguments\n",
    "4. Add results of tool call with corresponding ID to conversation\n",
    "5. Re-generate message with the updated conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute tool call in System / Global environment\n",
    "\n",
    "In the above workflow, the actual function can be retrieved from global environment via its name. However, this approach is not appropriate for functions imported from other modules or API requests. Therefore, we can consider dynamic imports / function extraction and wrapper functions. These can be managed in data defined in higher levels of tools schema - `metadata`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic imports\n",
    "\n",
    "We can dynamically import functions and modules with `importlib`. To achieve this, we need module source as string combined with function name. For example, the function `get_weather_information` defined locally can be imported from module `__main__`. Meanwhile, `show_doc` function can be dynamically imported from module `nbdev.showdoc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'city': 'New York', 'zip_code': None, 'temparature': 25, 'humidity': 80}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "\n",
    "# Example of dynamically calling the 'get_weather_information' function\n",
    "main_module = importlib.import_module('__main__')\n",
    "weather_function = getattr(main_module, 'get_weather_information')\n",
    "weather_function(**{'city': 'New York'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### get_weather_information\n",
       "\n",
       ">      get_weather_information (city:str, zip_code:Optional[str]=None)"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### get_weather_information\n",
       "\n",
       ">      get_weather_information (city:str, zip_code:Optional[str]=None)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of dynamically calling the 'show_doc' function\n",
    "showdoc_module = importlib.import_module('nbdev.showdoc')\n",
    "showdoc_function = getattr(showdoc_module, 'show_doc')\n",
    "showdoc_function(weather_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapper function\n",
    "\n",
    "In case the main function fails, we can execute resort to a wrapper function that also takes in high-level data. This should be useful for any scenarios that require additional information to execute or specific steps (e.g., API requests). For system design purpose, these fixup functions should include the following parameters:\n",
    "\n",
    "- **Function name**: Positional, required paramater  \n",
    "- **Metadata parameters**: Any high-level data as optional keyword arguments  \n",
    "- **Function parameters**: Function arguments provided by GPT as optional keyword arguments  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High-level data in tools schema\n",
    "\n",
    "High-level data should be stored as simple JSON-formatted data in tools schema such that it does not interfere with GPT argument-generating process. A suitable structure for this would be adding these information as properties in the function object of schema:\n",
    "\n",
    "- `metadata`: JSON-formatted data for any additional information  \n",
    "- `fixup`: Name and module source of fixup function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of tools schema with high-level data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tools schema high-level information\n",
    "tools = [{\n",
    "    'type': 'function',\n",
    "    'function': {\n",
    "        'name': 'get_weather_information',\n",
    "        'description': 'Get weather information for a given location',\n",
    "        'parameters': {\n",
    "            'type': 'object',\n",
    "            'properties': {\n",
    "                'city': {\n",
    "                    'type': 'string',\n",
    "                    'description': 'City name'\n",
    "                },\n",
    "                'zip_code': {\n",
    "                    'anyOf': [\n",
    "                        {'type': 'string'},\n",
    "                        {'type': 'null'},\n",
    "                    ],\n",
    "                },\n",
    "            },\n",
    "            'required': ['city'],\n",
    "        },\n",
    "        # Extra high-level information\n",
    "        'metadata': {\n",
    "            'module': '__main__',  # Module name\n",
    "        },\n",
    "        'fixup': 'fixup.module.function',  # Fixup function\n",
    "    }\n",
    "}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities function for managing messages\n",
    "\n",
    "This section implements some basic utilities for forming and printing messages to suitable formats used in conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import textwrap\n",
    "from colorama import Fore, Back, Style\n",
    "from typing import Literal, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def form_msg(\n",
    "    role: Literal[\"system\", \"user\", \"assistant\", \"tool\"],  # The role of the message sender\n",
    "    content: str,  # The content of the message\n",
    "    tool_call_id: Optional[str] = None,  # The ID of the tool call (if role == \"tool\")\n",
    "):\n",
    "    \"\"\"Create a message for the conversation\"\"\"\n",
    "    msg = {\n",
    "        \"role\": role,\n",
    "        \"content\": content\n",
    "    }\n",
    "    if role == \"tool\":\n",
    "        msg[\"tool_call_id\"] = tool_call_id\n",
    "    return msg\n",
    "\n",
    "def form_msgs(\n",
    "    msgs: list[tuple[Literal[\"system\", \"user\", \"assistant\"], str]]  # The list of messages to form in tuples of role and content\n",
    "): \n",
    "    \"\"\"Form a list of messages for the conversation\"\"\"\n",
    "    return [{\"role\":m[0],\"content\":m[1]} for m in msgs]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/ninjalabo/llmcam/blob/main/llmcam/core/fn_to_fc.py#L219){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### form_msg\n",
       "\n",
       ">      form_msg (role:Literal['system','user','assistant','tool'], content:str,\n",
       ">                tool_call_id:Optional[str]=None)\n",
       "\n",
       "*Create a message for the conversation*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| role | Literal |  | The role of the message sender |\n",
       "| content | str |  | The content of the message |\n",
       "| tool_call_id | Optional | None | The ID of the tool call (if role == \"tool\") |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/ninjalabo/llmcam/blob/main/llmcam/core/fn_to_fc.py#L219){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### form_msg\n",
       "\n",
       ">      form_msg (role:Literal['system','user','assistant','tool'], content:str,\n",
       ">                tool_call_id:Optional[str]=None)\n",
       "\n",
       "*Create a message for the conversation*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| role | Literal |  | The role of the message sender |\n",
       "| content | str |  | The content of the message |\n",
       "| tool_call_id | Optional | None | The ID of the tool call (if role == \"tool\") |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(form_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/ninjalabo/llmcam/blob/main/llmcam/core/fn_to_fc.py#L233){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### form_msgs\n",
       "\n",
       ">      form_msgs\n",
       ">                 (msgs:list[tuple[typing.Literal['system','user','assistant'],s\n",
       ">                 tr]])\n",
       "\n",
       "*Form a list of messages for the conversation*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| msgs | list | The list of messages to form in tuples of role and content |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/ninjalabo/llmcam/blob/main/llmcam/core/fn_to_fc.py#L233){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### form_msgs\n",
       "\n",
       ">      form_msgs\n",
       ">                 (msgs:list[tuple[typing.Literal['system','user','assistant'],s\n",
       ">                 tr]])\n",
       "\n",
       "*Form a list of messages for the conversation*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| msgs | list | The list of messages to form in tuples of role and content |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(form_msgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def print_msg(\n",
    "    msg: dict  # The message to print with role and content\n",
    "):\n",
    "    \"\"\"Print a message with role and content\"\"\"\n",
    "    who = msg['role'].capitalize()\n",
    "    who = (Fore.RED if who in \"System\" else Fore.GREEN if who in \"User\" else Fore.BLUE if who in \"Assistant\" else Fore.CYAN) + who\n",
    "    who = Back.YELLOW + who\n",
    "    print(Style.BRIGHT + Fore.RED + f\">> {who}:\" + Style.RESET_ALL)\n",
    "    try:\n",
    "        print(textwrap.fill(msg[\"content\"], 100))\n",
    "    except:\n",
    "        print(msg)\n",
    "\n",
    "def print_msgs(\n",
    "    msgs: list[dict],  # The list of messages to print with role and content\n",
    "    with_tool: bool = False  # Whether to print tool messages\n",
    "):\n",
    "    for msg in msgs:\n",
    "        if not with_tool and any(key in msg for key in ('tool_calls', 'tool_call_id')):\n",
    "            continue\n",
    "        print_msg(msg)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/ninjalabo/llmcam/blob/main/llmcam/core/fn_to_fc.py#L203){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### print_msg\n",
       "\n",
       ">      print_msg (msg:dict)\n",
       "\n",
       "*Print a message with role and content*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| msg | dict | The message to print with role and content |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/ninjalabo/llmcam/blob/main/llmcam/core/fn_to_fc.py#L203){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### print_msg\n",
       "\n",
       ">      print_msg (msg:dict)\n",
       "\n",
       "*Print a message with role and content*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| msg | dict | The message to print with role and content |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(print_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/ninjalabo/llmcam/blob/main/llmcam/core/fn_to_fc.py#L213){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### print_msgs\n",
       "\n",
       ">      print_msgs (msgs:list[dict], with_tool:bool=False)\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| msgs | list |  | The list of messages to print with role and content |\n",
       "| with_tool | bool | False | Whether to print tool messages |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/ninjalabo/llmcam/blob/main/llmcam/core/fn_to_fc.py#L213){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### print_msgs\n",
       "\n",
       ">      print_msgs (msgs:list[dict], with_tool:bool=False)\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| msgs | list |  | The list of messages to print with role and content |\n",
       "| with_tool | bool | False | Whether to print tool messages |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(print_msgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modularized execution function\n",
    "\n",
    "This section implements the described FC workflow in a thorough execution function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import importlib\n",
    "import json\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Support functions to handle tool response,where call == response.choices[0].message.tool_calls[i]\n",
    "def fn_name(call): return call[\"function\"][\"name\"]\n",
    "def fn_args(call): return json.loads(call[\"function\"][\"arguments\"])\n",
    "def fn_metadata(tool): return tool[\"function\"][\"metadata\"]\n",
    "\n",
    "def fn_exec(call, tools=[]):\n",
    "    \"\"\"Execute the function call\"\"\"\n",
    "    for tool in tools:\n",
    "        # Check if the function name matches\n",
    "        if call['function']['name'] != tool['function']['name']:\n",
    "            continue\n",
    "\n",
    "        # Execute the function by dynamically importing the module\n",
    "        try:\n",
    "            module_path = tool['function']['metadata']['module']\n",
    "            module = importlib.import_module(module_path)\n",
    "            fn = getattr(module, fn_name(call))\n",
    "            return fn(**fn_args(call))\n",
    "        \n",
    "        # If the function is not found, try to fix it\n",
    "        except Exception as e:\n",
    "            if not 'fixup' in tool['function']:\n",
    "                continue\n",
    "            module_path, fn_path = tool['function']['fixup'].rsplit('.', 1)\n",
    "            fn = getattr(importlib.import_module(module_path), fn_path)\n",
    "            return fn(fn_name(call), **fn_metadata(tool), **fn_args(call))\n",
    "\n",
    "def fn_result_content(call, tools=[]):\n",
    "    \"\"\"Create a content containing the result of the function call\"\"\"\n",
    "    content = dict()\n",
    "    content.update(fn_args(call))\n",
    "    content.update({fn_name(call): fn_exec(call, tools)})\n",
    "    return json.dumps(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def complete(\n",
    "        messages: list[dict],  # The list of messages\n",
    "        tools: list[dict] = [],  # The list of tools\n",
    "    ) -> tuple[str, str]:  # The role and content of the last message\n",
    "    \"\"\"Complete the conversation with the given message\"\"\"\n",
    "    # Generate the response from GPT-4\n",
    "    response = openai.chat.completions.create(model=\"gpt-4o\", messages=messages, tools=tools)\n",
    "    res = response.choices[0].message\n",
    "    messages.append(res.to_dict())\n",
    "\n",
    "    # Handle the tool response\n",
    "    for call in res.to_dict().get('tool_calls', []):\n",
    "        # Append the tool response to the list\n",
    "        messages.append(\n",
    "            form_msg(\n",
    "                role=\"tool\",\n",
    "                content=fn_result_content(call, tools=tools),\n",
    "                tool_call_id=call[\"id\"]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if res.to_dict().get('tool_calls'):\n",
    "        # Recursively call the complete function to handle the tool response\n",
    "        complete(\n",
    "            messages, \n",
    "            tools=tools\n",
    "        )\n",
    "\n",
    "    # Return the last message\n",
    "    return messages[-1]['role'], messages[-1]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/ninjalabo/llmcam/blob/main/llmcam/core/fn_to_fc.py#L236){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### complete\n",
       "\n",
       ">      complete (messages:list[dict], tools:list[dict]=[])\n",
       "\n",
       "*Complete the conversation with the given message*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| messages | list |  | The list of messages |\n",
       "| tools | list | [] | The list of tools |\n",
       "| **Returns** | **tuple** |  | **The role and content of the last message** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/ninjalabo/llmcam/blob/main/llmcam/core/fn_to_fc.py#L236){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### complete\n",
       "\n",
       ">      complete (messages:list[dict], tools:list[dict]=[])\n",
       "\n",
       "*Complete the conversation with the given message*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| messages | list |  | The list of messages |\n",
       "| tools | list | [] | The list of tools |\n",
       "| **Returns** | **tuple** |  | **The role and content of the last message** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(complete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test with the previously demonstrated example - using the `get_weather_information` function and updated `tools` with `metadata` containing the module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31m>> \u001b[43m\u001b[31mSystem:\u001b[0m\n",
      "You can get weather information for a given location using the `get_weather_information` function\n",
      "\u001b[1m\u001b[31m>> \u001b[43m\u001b[32mUser:\u001b[0m\n",
      "What is the weather in New York?\n",
      "\u001b[1m\u001b[31m>> \u001b[43m\u001b[34mAssistant:\u001b[0m\n",
      "{'content': None, 'refusal': None, 'role': 'assistant', 'tool_calls': [{'id': 'call_D819bD5iC06S3OmMGUIjuZi9', 'function': {'arguments': '{\"city\":\"New York\"}', 'name': 'get_weather_information'}, 'type': 'function'}]}\n",
      "\u001b[1m\u001b[31m>> \u001b[43m\u001b[36mTool:\u001b[0m\n",
      "{\"city\": \"New York\", \"get_weather_information\": {\"city\": \"New York\", \"zip_code\": null,\n",
      "\"temparature\": 25, \"humidity\": 80}}\n",
      "\u001b[1m\u001b[31m>> \u001b[43m\u001b[34mAssistant:\u001b[0m\n",
      "The current weather in New York is 25Â°C with a humidity level of 80%.\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "messages = form_msgs([\n",
    "    (\"system\", \"You can get weather information for a given location using the `get_weather_information` function\"),\n",
    "    (\"user\", \"What is the weather in New York?\")\n",
    "])\n",
    "complete(messages, tools=tools)\n",
    "print_msgs(messages, with_tool=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
