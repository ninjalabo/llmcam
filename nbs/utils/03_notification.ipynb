{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notification\n",
    "\n",
    "> Python module to implement notification workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp utils.notification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nested GPT Function calling\n",
    "\n",
    "Sending notifications is an important feature for our application. However, because this monitoring process should be initiated by the user via Chat GPT, the condition for sending notifications or stopping the monitoring stream is not constant. Therefore, we can attempt dynamically checking the conditions by using nested GPT Function calling.\n",
    "\n",
    "In nested GPT FC, the notification process will start an on-going loop for checking the condition for sending notification. The condition itself is not explicitly hard-coded in the code but decided by a sub GPT Agent in the loop. This sub GPT FC process will take in the previous messages which contain the description for the notification task. This sub GPT will also be provided with another tool for sending notifications, which will be called if the condition as described is satisfied in the current loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"display: flex; justify-content: center; align-items: center; width: 100%; height: 100%;\">\n",
       "        <img src=\"https://mermaid.ink/img/CmZsb3djaGFydCBURAogICAgVVtVc2VyXSAtLT58bm90aWZpY2F0aW9uIHRhc2sgZGVzY3JpcHRpb258IE1HW01haW4gR1BUXQogICAgTUctLT58c3RhcnQgc2VwYXJhdGUgdGhyZWFkfCBTW05vdGlmaWNhdGlvbiBTdHJlYW1dCiAgICBOU0ZbTm90aWZpY2F0aW9uIHNlbmRlciBhcyBHUFQtY29tcGF0aWJsZWZ1bmN0aW9uXSAtLT58dG9vbHwgU0cKICAgIFNGW1N0cmVhbSBzdG9wcGVyIGFzIEdQVC1jb21wYXRpYmxlIGZ1bmN0aW9uXSAtLT58dG9vbHwgU0cKICAgIE5TRiAtLT58c2VuZCBub3RpZmljYXRpb258IFUKICAgIHN1YmdyYXBoIE5vdGlmaWNhdGlvbiBzdHJlYW0KICAgICAgICBTIC0tPnxwYXNzIGRlc2NyaXB0aW9ufCBTR1tTdWIgR1BUXQogICAgICAgIFNHIC0tPnxjaGVjayBjb25kaXRpb24gYXMgZGVzY3JpYmVkfCBTRwogICAgICAgIFNHIC0tPnxzZW5kIGNvbmRpdGlvbiBtZXR8IE5TRgogICAgICAgIFNHIC0tPnxzdG9wIGNvbmRpdGlvbiBtZXR8IFNGCiAgICAgICAgU0cgLS0-fGxvb3B8IFMKICAgIGVuZAo=\" style=\"max-width: 100%; max-height: 100%; object-fit: contain;\" />\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| echo: false\n",
    "import base64\n",
    "from IPython.display import HTML, display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def mm(graph):\n",
    "    graphbytes = graph.encode(\"utf8\")\n",
    "    base64_bytes = base64.urlsafe_b64encode(graphbytes)\n",
    "    base64_string = base64_bytes.decode(\"ascii\")\n",
    "    img_url = \"https://mermaid.ink/img/\" + base64_string\n",
    "    \n",
    "    # Responsive HTML with CSS for fitting to parent container\n",
    "    html = f\"\"\"\n",
    "    <div style=\"display: flex; justify-content: center; align-items: center; width: 100%; height: 100%;\">\n",
    "        <img src=\"{img_url}\" style=\"max-width: 100%; max-height: 100%; object-fit: contain;\" />\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    display(HTML(html))\n",
    "\n",
    "mm(\"\"\"\n",
    "flowchart TD\n",
    "    U[User] -->|notification task description| MG[Main GPT]\n",
    "    MG-->|start separate thread| S[Notification Stream]\n",
    "    NSF[Notification sender as GPT-compatiblefunction] -->|tool| SG\n",
    "    SF[Stream stopper as GPT-compatible function] -->|tool| SG\n",
    "    NSF -->|send notification| U\n",
    "    subgraph Notification stream\n",
    "        S -->|pass description| SG[Sub GPT]\n",
    "        SG -->|check condition as described| SG\n",
    "        SG -->|send condition met| NSF\n",
    "        SG -->|stop condition met| SF\n",
    "        SG -->|loop| S\n",
    "    end\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example scenario is presented as below. Supposing that we have a function (tool) for generating random integers between 1 and 100. We want to be notified every time the function generates a number higher than 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from llmcam.core.fc import *\n",
    "from llmcam.core.fn_to_schema import function_schema\n",
    "\n",
    "def random_generator():\n",
    "    \"\"\"Generate a random number between 1 and 100\"\"\"\n",
    "    return random.randint(1, 100)\n",
    "\n",
    "tools = [function_schema(random_generator)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define tool for sending notifications. In this simplified context, this function only adds the notification message to an existing list named `notifications`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notifications = []\n",
    "def send_notification(msg: str):\n",
    "    \"\"\"Send a notification\"\"\"\n",
    "    notifications.append(msg)\n",
    "    return notifications "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test function for starting the monitoring stream:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def start_notification_stream(\n",
    "    messages: list  # Previous conversation with the user\n",
    "):\n",
    "    \"\"\"Start a notification stream\"\"\"\n",
    "    subtools = [ tool for tool in tools ]\n",
    "    subtools.append(function_schema(send_notification))\n",
    "\n",
    "    for _ in range(5):  # Only loop up to 5 times for the demo\n",
    "        complete(messages, tools=subtools)\n",
    "        time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "start_notification_stream(\n",
    "    messages = form_msgs([\n",
    "        ('user', 'Can you notify me every time you generate a number higher than 50? Stop after 10 notifications.')\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Generated number is 61, which is higher than 50.',\n",
       " 'Generated number is 94, which is higher than 50.',\n",
       " 'Generated number is 86, which is higher than 50.',\n",
       " 'Generated number is 59, which is higher than 50.',\n",
       " 'Generated number is 100, which is higher than 50.',\n",
       " 'Generated number is 58, which is higher than 50.',\n",
       " 'Generated number is 88, which is higher than 50.',\n",
       " 'Generated number is 82, which is higher than 50.',\n",
       " 'Generated number is 73, which is higher than 50.',\n",
       " 'Generated number is 84, which is higher than 50.']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: False\n",
    "notifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modularized notification workflow with separated StreamThread\n",
    "\n",
    "In the demo above, this function is not yet compatible with our current GPT FC framework. One approach to modularize this function and incorporate parallel execution is to use Python `threading` library. We can define a custom `Thread` class to initiate the monitoring process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from threading import Thread, Event\n",
    "import time\n",
    "from typing import Optional, Callable\n",
    "from llmcam.core.fc import *\n",
    "from llmcam.core.fn_to_schema import function_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Define the stream thread class\n",
    "class StreamThread(Thread):\n",
    "    \"\"\"A class to run a notification stream in a separate thread\"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        thread_id: int,  # The thread ID \n",
    "        tools: list,  # List of tools for sub GPT \n",
    "        messages: list  # Previous conversation with the user\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.thread_id = thread_id\n",
    "        self.stop_event = Event()\n",
    "        self.tools = tools\n",
    "        self.messages = messages\n",
    "\n",
    "    def run(self):\n",
    "        while not self.stop_event.is_set():\n",
    "            complete(self.messages, tools=self.tools)\n",
    "            time.sleep(5)\n",
    "\n",
    "    def stop(self):\n",
    "        self.stop_event.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/ninjalabo/llmcam/blob/main/llmcam/utils/notification.py#L17){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### StreamThread\n",
       "\n",
       ">      StreamThread (thread_id:int, tools:list, messages:list)\n",
       "\n",
       "*A class to run a notification stream in a separate thread*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| thread_id | int | The thread ID |\n",
       "| tools | list | List of tools for sub GPT |\n",
       "| messages | list | Previous conversation with the user |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/ninjalabo/llmcam/blob/main/llmcam/utils/notification.py#L17){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### StreamThread\n",
       "\n",
       ">      StreamThread (thread_id:int, tools:list, messages:list)\n",
       "\n",
       "*A class to run a notification stream in a separate thread*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| thread_id | int | The thread ID |\n",
       "| tools | list | List of tools for sub GPT |\n",
       "| messages | list | Previous conversation with the user |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(StreamThread)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from a function to send notifications, we may also need a function to halt the notification stream. This function may heavily depend on the context of usage (e.g., scripts, web services, ...). Therefore, we can define a pair of starting and stopping a stream thread, with an example being the default functions that can be easily used in a Python script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def default_stream_starter(tools, messages):\n",
    "    \"\"\"Default function to start the notifications stream\"\"\"\n",
    "    global stream_thread\n",
    "\n",
    "    # Start the notifications stream\n",
    "    stream_thread = StreamThread(1, tools, messages)\n",
    "    stream_thread.start()\n",
    "\n",
    "def default_stream_stopper():\n",
    "    \"\"\"Default function to stop the notifications stream\"\"\"\n",
    "    global stream_thread\n",
    "\n",
    "    # Stop the notifications stream\n",
    "    stream_thread.stop()\n",
    "    stream_thread.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to `store` utilities, the function for starting a notification stream needs to be attached to multiple context-dependent variables (tools, stream starter and stopper, notifications sender, ...). Therefore, we need to define the `_core` function rather than a hard-coded stream function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def notification_stream_core(\n",
    "    tools: list,  # Tools to use\n",
    "    messages: list,  # Previous conversation with the user\n",
    "    stream_starter: Optional[Callable] = None,  # Function to start the stream\n",
    "    send_notification: Optional[Callable] = None,  # Function to send the notification\n",
    "    stream_stopper: Optional[Callable] = None,  # Function to stop the stream\n",
    "    send_notification_schema: Optional[dict] = None,  # Schema for the send_notification function\n",
    "    stream_stopper_schema: Optional[dict] = None,  # Schema for the stream_stopper function\n",
    ") -> str:\n",
    "    \"\"\"Core function to start and stop the notifications stream\"\"\"\n",
    "    # Copy the messages to avoid modifying the original list\n",
    "    submessages = [ message for message in messages ]\n",
    "\n",
    "    # Extract subtools schemas\n",
    "    send_notification_schema = send_notification_schema or function_schema(send_notification, 'send_notification')\n",
    "    stream_stopper_schema = stream_stopper_schema or function_schema(stream_stopper, 'send_notification')\n",
    "\n",
    "    # Add sending notification services to tool schema\n",
    "    subtools = [ tool for tool in tools if tool['function']['name'] != 'start_notification_stream' ]\n",
    "    subtools.append(send_notification_schema)\n",
    "    subtools.append(stream_stopper_schema)\n",
    "\n",
    "    # Start the notifications stream\n",
    "    stream_starter(subtools, submessages)\n",
    "\n",
    "    return 'Notifications stream started'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An issue with the current `llmcam.core.fn_to_schema` is that the type suggestion does not cover complicated nested typings. In this case, the list of messages should be of a specific format to be inputted to OpenAI API: \n",
    "\n",
    "```\n",
    "[\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Can you notify me if ...\"\n",
    "    }\n",
    "]\n",
    "```\n",
    "Hence, the current work-around this issue is to manually adjust the schema of `start_notification_stream` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def process_notification_schema(\n",
    "    start_notifications_stream: Callable,  # Function to start the notifications stream\n",
    "):\n",
    "    \"\"\"Process the notification schema\"\"\"\n",
    "    notification_schema = function_schema(start_notifications_stream, 'notification')\n",
    "\n",
    "    notification_schema['function']['parameters'] = {\n",
    "        'type': 'object',\n",
    "        'properties': {\n",
    "            'messages': {\n",
    "                'description': 'All the previous messages in the conversation',\n",
    "                'type': 'array',\n",
    "                'items': {\n",
    "                    'type': 'object',\n",
    "                    'properties': {\n",
    "                        'role': {\n",
    "                            'type': 'string',\n",
    "                            'enum': ['user', 'tool', 'system', 'assistant']\n",
    "                        },\n",
    "                        'content': {\n",
    "                            'type': 'string'\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    return notification_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulated GPT workflow\n",
    "\n",
    "Test integrating with our current GPT framework. This process follows the above demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "import random\n",
    "from llmcam.core.fc import *\n",
    "from llmcam.core.fn_to_schema import function_schema\n",
    "\n",
    "def random_generator():\n",
    "    \"\"\"Generate a random number between 1 and 100\"\"\"\n",
    "    return random.randint(1, 100)\n",
    "\n",
    "tools = [function_schema(random_generator)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "notifications = []\n",
    "def send_notification(msg: str):\n",
    "    \"\"\"Send a notification\"\"\"\n",
    "    notifications.append(msg)\n",
    "    return notifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "def start_notification_stream(\n",
    "    messages: list  # Previous conversation with the user\n",
    "):\n",
    "    return notification_stream_core(\n",
    "        tools, \n",
    "        messages,\n",
    "        stream_starter=default_stream_starter,\n",
    "        send_notification=send_notification,\n",
    "        stream_stopper=default_stream_stopper\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "tools.append(process_notification_schema(start_notification_stream))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31m>> \u001b[43m\u001b[31mSystem:\u001b[0m\n",
      "You are a helpful system administrator. Use the supplied tools to assist the user. If asked to\n",
      "monitor and notify, do not resolve in main thread but defer to sub GPT in notifcation stream\n",
      "instead.\n",
      "\u001b[1m\u001b[31m>> \u001b[43m\u001b[32mUser:\u001b[0m\n",
      "Can you notify me every time you generate a number higher than 50? Stop after 10 notifications.\n",
      "\u001b[1m\u001b[31m>> \u001b[43m\u001b[34mAssistant:\u001b[0m\n",
      "Sure, I'll begin generating random numbers and will notify you each time a number higher than 50 is\n",
      "generated, up to a total of 10 notifications. The process is underway.\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "messages = form_msgs([\n",
    "    ('system', 'You are a helpful system administrator. Use the supplied tools to assist the user. \\\n",
    "If asked to monitor and notify, do not resolve in main thread but defer to sub GPT in notifcation stream instead.'),\n",
    "    ('user', 'Can you notify me every time you generate a number higher than 50? Stop after 10 notifications.'),\n",
    "])\n",
    "complete(messages, tools=tools)\n",
    "print_msgs(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the notifications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated a number higher than 50: 53\n",
      "Generated a number higher than 50: 63\n",
      "Generated a number higher than 50: 97\n",
      "Generated a number higher than 50: 82\n",
      "Generated a number higher than 50: 65\n",
      "Generated a number higher than 50: 82\n",
      "Generated a number higher than 50: 74\n",
      "Generated a number higher than 50: 98\n",
      "Generated a number higher than 50: 82\n",
      "Generated a number higher than 50: 86\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "for noti in notifications:\n",
    "    print(noti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "len(notifications)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the global stream thread:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "stream_thread.is_alive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
