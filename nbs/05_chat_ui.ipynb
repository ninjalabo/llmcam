{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36f0ff07-13fe-4ebe-970d-4848f7462e09",
   "metadata": {},
   "source": [
    "# chat UI in fastHTML\n",
    "> chat UI implemented in fastHTML\n",
    "\n",
    "You can start to implement fake chat log generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b36b977-f531-4d05-af7a-1761ef5d2683",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp chat_ui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddd185b2-ec84-46d1-8474-9038ee3eb232",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6714c4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import uvicorn\n",
    "import importlib.util\n",
    "from fasthtml.common import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2a5b1df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Set up the app, including daisyui and tailwind for the chat component\n",
    "hdrs = (picolink, Script(src=\"https://cdn.tailwindcss.com\"),\n",
    "        Link(rel=\"stylesheet\", href=\"https://cdn.jsdelivr.net/npm/daisyui@4.11.1/dist/full.min.css\"))\n",
    "app = FastHTML(hdrs=hdrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0211a31",
   "metadata": {},
   "source": [
    "## Chat components\n",
    "\n",
    "Basic chat UI components can include Chat Message and a Chat Input. For a Chat Message, the important attributes are the actual message (str) and the role of the message owner (user - boolean value whether the owner is the user, not the AI assistant)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a6c7f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Chat message component (renders a chat bubble)\n",
    "def ChatMessage(\n",
    "        msg: str,  # Message to display\n",
    "        user: bool  # Whether the message is from the user or assistant\n",
    "    ):  # Returns a Div containing the chat bubble\n",
    "    paragraphs = msg.split(\"\\n\")\n",
    "    # Set class to change displayed style of bubble\n",
    "    bubble_class = \"chat-bubble-primary\" if user else 'chat-bubble-secondary'\n",
    "    chat_class = \"chat-end\" if user else 'chat-start'\n",
    "    return  Div(cls=f\"chat {chat_class}\")(\n",
    "                Div('User' if user else 'Assistant', cls=\"chat-header\"),\n",
    "                Div(\n",
    "                    *[P(p, style=f\"color: {'black' if user else 'white'};\") for p in paragraphs], \n",
    "                    cls=f\"chat-bubble {bubble_class}\", \n",
    "                    style=f\"background-color: {'#03fcad' if user else '#025238'};\"),\n",
    "                Hidden(msg, name=\"messages\"),  # Hidden field for submitting past messages to form\n",
    "                Hidden(\"user\" if user else \"assistant\", name=\"roles\")  # Hidden field for submitting corresponding owners\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61d96926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/ninjalabo/llmcam/blob/main/llmcam/chat_ui.py#L21){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ChatMessage\n",
       "\n",
       ">      ChatMessage (msg:str, user:bool)\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| msg | str | Message to display |\n",
       "| user | bool | Whether the message is from the user or assistant |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/ninjalabo/llmcam/blob/main/llmcam/chat_ui.py#L21){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ChatMessage\n",
       "\n",
       ">      ChatMessage (msg:str, user:bool)\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| msg | str | Message to display |\n",
       "| user | bool | Whether the message is from the user or assistant |"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(ChatMessage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc84d14",
   "metadata": {},
   "source": [
    "For the chat input, set the name for submitting a new message via form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b6bbd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# The input field for the user message. Also used to clear the\n",
    "# input field after sending a message via an OOB swap\n",
    "def ChatInput():  # Returns an input field for the user message\n",
    "    return Input(name='msg', id='msg-input', placeholder=\"Type a message\",\n",
    "                 cls=\"input input-bordered w-full rounded-l-2xl bg-stone-800\", \n",
    "                 hx_swap_oob='true'  # Re-render the element to remove submitted message\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad8a8814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/ninjalabo/llmcam/blob/main/llmcam/chat_ui.py#L39){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ChatInput\n",
       "\n",
       ">      ChatInput ()"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/ninjalabo/llmcam/blob/main/llmcam/chat_ui.py#L39){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ChatInput\n",
       "\n",
       ">      ChatInput ()"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(ChatInput)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7986a51e",
   "metadata": {},
   "source": [
    "### Action Buttons\n",
    "\n",
    "Simple actions for creating a new message from the user side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "86cba3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def ActionButton(\n",
    "        content: str  # Text to display on the button\n",
    "    ):  # Returns a button with the given content\n",
    "\n",
    "    return Form(\n",
    "        hx_post=\"/\",\n",
    "        hx_target=\"#chatlist\",\n",
    "        hx_swap=\"beforeend\",  # Location: just before the end of element\n",
    "    )(\n",
    "        Hidden(content, name=\"msg\"),\n",
    "        Button(content, cls=\"btn btn-secondary\")\n",
    "    )\n",
    "\n",
    "def ActionPanel():  # Returns a panel of action buttons\n",
    "    return Div(\n",
    "        ActionButton(\"Do something\"),\n",
    "        ActionButton(\"A different action\"),\n",
    "        cls=\"flex flex-row h-fit px-24 gap-4 pt-4\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe59ea66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/ninjalabo/llmcam/blob/main/llmcam/chat_ui.py#L46){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ActionButton\n",
       "\n",
       ">      ActionButton (content:str)\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| content | str | Text to display on the button |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/ninjalabo/llmcam/blob/main/llmcam/chat_ui.py#L46){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ActionButton\n",
       "\n",
       ">      ActionButton (content:str)\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| content | str | Text to display on the button |"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(ActionButton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ef9f47bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/ninjalabo/llmcam/blob/main/llmcam/chat_ui.py#L56){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ActionPanel\n",
       "\n",
       ">      ActionPanel ()"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/ninjalabo/llmcam/blob/main/llmcam/chat_ui.py#L56){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ActionPanel\n",
       "\n",
       ">      ActionPanel ()"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(ActionPanel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174209ca",
   "metadata": {},
   "source": [
    "## Router"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01b9a76",
   "metadata": {},
   "source": [
    "### Home page\n",
    "The home page should contain our message list and the Chat Input. The main page can be extracted by accessing the index (root) endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3371d10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@app.get('/')\n",
    "def index():\n",
    "    sidebar = Div(\n",
    "        H1(\"Conversations\"),\n",
    "        cls=\"w-[30vw] bg-stone-800\"\n",
    "    )\n",
    "    page =  Div(cls=\"w-full flex flex-col p-0\")(\n",
    "        ActionPanel(),\n",
    "        Form(\n",
    "            hx_post=\"/\",  # Operation: some POST endpoint with function `send` \n",
    "            hx_target=\"#chatlist\",  # Target: element with ID 'chatlist'\n",
    "            hx_swap=\"beforeend\",  # Location: just before the end of element\n",
    "            cls=\"w-full flex flex-col px-24 h-[90vh]\"\n",
    "        )(\n",
    "            # The chat list\n",
    "            Div(id=\"chatlist\", cls=\"chat-box overflow-y-auto flex-1 w-full mt-10\")(\n",
    "                # One initial message from AI assistant\n",
    "                ChatMessage(\"Hello! I'm a chatbot. How can I help you today?\", False),\n",
    "            ),\n",
    "            # Input form\n",
    "            Div(cls=\"h-fit mb-5 mt-5 flex space-x-2 mt-2\")(\n",
    "                Group(\n",
    "                    ChatInput(), \n",
    "                    Button(\"Send\", cls=\"btn btn-primary rounded-r-2xl\", style=\"background-color: #03fcad;\"))\n",
    "            )\n",
    "        )   \n",
    "    )\n",
    "    return Main(\n",
    "        sidebar,\n",
    "        page, \n",
    "        data_theme=\"forest\", \n",
    "        cls=\"h-[100vh] w-full relative flex flex-row items-stretch overflow-hidden transition-colors z-0 p-0\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f987f6",
   "metadata": {},
   "source": [
    "### Form submission\n",
    "\n",
    "At submission, this function should:\n",
    "\n",
    "- Extract the new and all previous chat history  \n",
    "- Prompt & get answers from ChatGPT from all these messages  \n",
    "- Return a new ChatMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "74c803cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Handle the form submission\n",
    "@app.post('/')\n",
    "def send(msg: str, messages: list[str] = None, roles: list[str] = None):\n",
    "    if not messages: messages = []\n",
    "    if not roles: roles = []\n",
    "\n",
    "    # Format the prompt for ChatGPT\n",
    "    prompt = [ {\"role\": roles[i], \"content\": messages[i]} for i in range(len(messages)) ]\n",
    "    \n",
    "    # Add the user message to the prompt\n",
    "    prompt.append({\"role\": \"user\", \"content\": msg})\n",
    "    \n",
    "    # Get the response from ChatGPT\n",
    "    response = \"Response cannot be generated at the moment, please comeback later.\"\n",
    "    \n",
    "    return (ChatMessage(msg, True),    # The user's message\n",
    "            ChatMessage(response, False), # The chatbot's response\n",
    "            ChatInput()) # And clear the input field via an OOB swap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3dc6cb",
   "metadata": {},
   "source": [
    "## Runner\n",
    "\n",
    "In addition to the main app, an utility function is implemented to run the app just by importing and executing this function to a Python file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7a8309b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def llmcam_chatbot(\n",
    "        package_name=\"ninjalabo.llmcam\",  # The installed package name\n",
    "        module_name=\"chat_ui\",  # The module containing the FastAPI app\n",
    "        app_variable=\"app\",  # The FastAPI app variable name\n",
    "        host=\"0.0.0.0\",  # The host to listen on\n",
    "        port=5001,  # The port to listen on\n",
    "        **uvicorn_kwargs  # Additional keyword arguments for uvicorn\n",
    "    ):\n",
    "    \"Find and run the FastAPI app in the specified module within the given package.\"\n",
    "    # Construct the full module path (e.g., 'llmcam.chat_ui')\n",
    "    full_module_path = f\"{package_name.split('.')[-1]}.{module_name}\"\n",
    "\n",
    "    # Check if the module exists in the installed package\n",
    "    try:\n",
    "        spec = importlib.util.find_spec(full_module_path)\n",
    "        if spec is None:\n",
    "            print(f\"Module '{full_module_path}' not found in package '{package_name}'.\")\n",
    "            return\n",
    "        # Dynamically run the Uvicorn server\n",
    "        uvicorn.run(f\"{full_module_path}:{app_variable}\", host=host, port=port, **uvicorn_kwargs)\n",
    "    except Exception as e:\n",
    "        print(f\"Error running the app: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d21e80f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/ninjalabo/llmcam/blob/main/llmcam/chat_ui.py#L118){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### llmcam_chatbot\n",
       "\n",
       ">      llmcam_chatbot (package_name='ninjalabo.llmcam', module_name='chat_ui',\n",
       ">                      app_variable='app', host='0.0.0.0', port=5001,\n",
       ">                      **uvicorn_kwargs)\n",
       "\n",
       "*Find and run the FastAPI app in the specified module within the given package.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| package_name | str | ninjalabo.llmcam | The installed package name |\n",
       "| module_name | str | chat_ui | The module containing the FastAPI app |\n",
       "| app_variable | str | app | The FastAPI app variable name |\n",
       "| host | str | 0.0.0.0 | The host to listen on |\n",
       "| port | int | 5001 | The port to listen on |\n",
       "| uvicorn_kwargs |  |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/ninjalabo/llmcam/blob/main/llmcam/chat_ui.py#L118){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### llmcam_chatbot\n",
       "\n",
       ">      llmcam_chatbot (package_name='ninjalabo.llmcam', module_name='chat_ui',\n",
       ">                      app_variable='app', host='0.0.0.0', port=5001,\n",
       ">                      **uvicorn_kwargs)\n",
       "\n",
       "*Find and run the FastAPI app in the specified module within the given package.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| package_name | str | ninjalabo.llmcam | The installed package name |\n",
       "| module_name | str | chat_ui | The module containing the FastAPI app |\n",
       "| app_variable | str | app | The FastAPI app variable name |\n",
       "| host | str | 0.0.0.0 | The host to listen on |\n",
       "| port | int | 5001 | The port to listen on |\n",
       "| uvicorn_kwargs |  |  |  |"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(llmcam_chatbot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7497fe71",
   "metadata": {},
   "source": [
    "For running while testing with Jupyter notebook, use the `JupyUvi` in `fasthtml` to run in separate thread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5d2e1b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "from fasthtml.jupyter import *\n",
    "\n",
    "server = JupyUvi(app=app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e61d3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "server.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4b67952f-5064-48ad-a0f5-a141be065b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fasthtml-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
