{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36f0ff07-13fe-4ebe-970d-4848f7462e09",
   "metadata": {},
   "source": [
    "# chat UI in fastHTML\n",
    "\n",
    "> chat UI implemented in fastHTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b36b977-f531-4d05-af7a-1761ef5d2683",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp chat_ui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd185b2-ec84-46d1-8474-9038ee3eb232",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd17c22c",
   "metadata": {},
   "source": [
    "## Chat App initialization\n",
    "\n",
    "Start by creating the chat application with `FastHTML`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6714c4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import uvicorn\n",
    "import importlib.util\n",
    "from fasthtml.common import *\n",
    "from llmcam.fn_to_fc import capture_youtube_live_frame_and_save, ask_gpt4v_about_image_file\n",
    "from llmcam.fn_to_fc import tool_schema, complete, form_msg\n",
    "from llmcam.store import add_api_tools, add_function_tools, remove_tools\n",
    "from llmcam.store import execute_handler_core, initialize_handlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ab256c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Set up base youtube live tools and session tools as simplified database\n",
    "YTLiveTools = [tool_schema(fn) for fn in (capture_youtube_live_frame_and_save, ask_gpt4v_about_image_file)]\n",
    "session_tools = {}\n",
    "def execute_handler(function_name, **kwargs):\n",
    "    return execute_handler_core(session_tools, function_name, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5b1df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Set up the app, including daisyui and tailwind for the chat component\n",
    "hdrs = (picolink, Script(src=\"https://cdn.tailwindcss.com\"),\n",
    "        Link(rel=\"stylesheet\", href=\"https://cdn.jsdelivr.net/npm/daisyui@4.11.1/dist/full.min.css\"),\n",
    "        MarkdownJS(), HighlightJS(langs=['python', 'javascript', 'html', 'css']))\n",
    "app = FastHTML(hdrs=hdrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0211a31",
   "metadata": {},
   "source": [
    "## Chat components\n",
    "\n",
    "Basic chat UI components can include Chat Message and a Chat Input. For a Chat Message, the important attributes are the actual message (str) and the role of the message owner (user - boolean value whether the owner is the user, not the AI assistant)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6c7f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Chat message component (renders a chat bubble)\n",
    "def ChatMessage(\n",
    "        msg: str,  # Message to display\n",
    "        user: bool  # Whether the message is from the user or assistant\n",
    "    ):  # Returns a Div containing the chat bubble\n",
    "    # Set class to change displayed style of bubble\n",
    "    bubble_class = \"chat-bubble-primary\" if user else 'chat-bubble-secondary'\n",
    "    chat_class = \"chat-end\" if user else 'chat-start'\n",
    "    return  Div(cls=f\"chat {chat_class}\")(\n",
    "                Div('User' if user else 'Assistant', cls=\"chat-header\"),\n",
    "                Div(\n",
    "                    msg,\n",
    "                    cls=f\"chat-bubble {bubble_class} marked px-6 py-4\", \n",
    "                    style=f\"background-color: {'#038a5e' if user else '#025238'}; color: {'black' if user else 'white'};\"),\n",
    "                Hidden(msg, name=\"contents\"),  # Hidden field for submitting past contents to form\n",
    "                Hidden(\"user\" if user else \"assistant\", name=\"roles\")  # Hidden field for submitting corresponding owners\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d96926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/ninjalabo/llmcam/blob/main/llmcam/chat_ui.py#L27){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ChatMessage\n",
       "\n",
       ">      ChatMessage (msg:str, user:bool)\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| msg | str | Message to display |\n",
       "| user | bool | Whether the message is from the user or assistant |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/ninjalabo/llmcam/blob/main/llmcam/chat_ui.py#L27){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ChatMessage\n",
       "\n",
       ">      ChatMessage (msg:str, user:bool)\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| msg | str | Message to display |\n",
       "| user | bool | Whether the message is from the user or assistant |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(ChatMessage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc84d14",
   "metadata": {},
   "source": [
    "For the chat input, set the name for submitting a new message via form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6bbd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# The input field for the user message. Also used to clear the\n",
    "# input field after sending a message via an OOB swap\n",
    "def ChatInput():  # Returns an input field for the user message\n",
    "    return Input(name='msg', id='msg-input', placeholder=\"Type a message\",\n",
    "                 cls=\"input input-bordered w-full rounded-l-2xl bg-stone-800\", \n",
    "                 hx_swap_oob='true'  # Re-render the element to remove submitted message\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8a8814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/ninjalabo/llmcam/blob/main/llmcam/chat_ui.py#L47){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ChatInput\n",
       "\n",
       ">      ChatInput ()"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/ninjalabo/llmcam/blob/main/llmcam/chat_ui.py#L47){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ChatInput\n",
       "\n",
       ">      ChatInput ()"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(ChatInput)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7986a51e",
   "metadata": {},
   "source": [
    "### Action Buttons\n",
    "\n",
    "Simple actions for creating a new message from the user side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cba3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def ActionButton(\n",
    "        content: str  # Text to display on the button\n",
    "    ):  # Returns a button with the given content\n",
    "\n",
    "    return Form(\n",
    "        hx_post=\"/\",\n",
    "        hx_target=\"#chatlist\",\n",
    "        hx_swap=\"beforeend\",  # Location: just before the end of element\n",
    "    )(\n",
    "        Hidden(content, name=\"msg\"),\n",
    "        Button(content, cls=\"btn btn-secondary\")\n",
    "    )\n",
    "\n",
    "def ActionPanel():  # Returns a panel of action buttons\n",
    "    return Div(\n",
    "        ActionButton(\"Introduce your model GPT-4o\"),\n",
    "        ActionButton(\"Extract information from a YouTube Live\"),\n",
    "        cls=\"flex flex-row h-fit px-24 gap-4 pt-4\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe59ea66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/ninjalabo/llmcam/blob/main/llmcam/chat_ui.py#L54){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ActionButton\n",
       "\n",
       ">      ActionButton (content:str)\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| content | str | Text to display on the button |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/ninjalabo/llmcam/blob/main/llmcam/chat_ui.py#L54){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ActionButton\n",
       "\n",
       ">      ActionButton (content:str)\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| content | str | Text to display on the button |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(ActionButton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9f47bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/ninjalabo/llmcam/blob/main/llmcam/chat_ui.py#L67){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ActionPanel\n",
       "\n",
       ">      ActionPanel ()"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/ninjalabo/llmcam/blob/main/llmcam/chat_ui.py#L67){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ActionPanel\n",
       "\n",
       ">      ActionPanel ()"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(ActionPanel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174209ca",
   "metadata": {},
   "source": [
    "## Router"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01b9a76",
   "metadata": {},
   "source": [
    "### Home page\n",
    "The home page should contain our message list and the Chat Input. The main page can be extracted by accessing the index (root) endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de53dced",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "scroll_script = Script(\"\"\"\n",
    "  // Function to scroll to the bottom of an element\n",
    "  function scrollToBottom(element) {\n",
    "    element.scrollTop = element.scrollHeight;\n",
    "  }\n",
    "\n",
    "  // Reference the expanding element\n",
    "  const expandingElement = document.getElementById('chatlist');\n",
    "\n",
    "  // Observe changes to the element's content and scroll down automatically\n",
    "  const observer = new MutationObserver(() => {\n",
    "    scrollToBottom(expandingElement);\n",
    "  });\n",
    "\n",
    "  // Start observing the expanding element for changes\n",
    "  observer.observe(expandingElement, { childList: true, subtree: true });\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3371d10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@app.get('/')\n",
    "def index():\n",
    "    sidebar = Div(\n",
    "        H1(\"Conversations\"),\n",
    "        cls=\"w-[30vw] bg-stone-800\"\n",
    "    )\n",
    "    page =  Div(cls=\"w-full flex flex-col p-0\")(\n",
    "        ActionPanel(),\n",
    "        Form(\n",
    "            hx_post=\"/\",  # Operation: some POST endpoint with function `send` \n",
    "            hx_target=\"#chatlist\",  # Target: element with ID 'chatlist'\n",
    "            hx_swap=\"beforeend\",  # Location: just before the end of element\n",
    "            cls=\"w-full flex flex-col px-24 h-[90vh]\"\n",
    "        )(\n",
    "            # The chat list\n",
    "            Div(id=\"chatlist\", cls=\"chat-box overflow-y-auto flex-1 w-full mt-10\")(\n",
    "                # One initial message from AI assistant\n",
    "                ChatMessage(\"Hello! I'm a chatbot. How can I help you today?\", False),\n",
    "            ),\n",
    "            # Input form\n",
    "            Div(cls=\"h-fit mb-5 mt-5 flex space-x-2 mt-2\")(\n",
    "                Group(\n",
    "                    ChatInput(), \n",
    "                    Button(\"Send\", cls=\"btn btn-primary rounded-r-2xl\", style=\"background-color: #03fcad;\"))\n",
    "            ),\n",
    "            scroll_script\n",
    "        )   \n",
    "    )\n",
    "    return Main(\n",
    "        sidebar,\n",
    "        page, \n",
    "        data_theme=\"forest\", \n",
    "        cls=\"h-[100vh] w-full relative flex flex-row items-stretch overflow-hidden transition-colors z-0 p-0\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f987f6",
   "metadata": {},
   "source": [
    "### Form submission\n",
    "\n",
    "At submission, this function should:\n",
    "\n",
    "- Extract the new and all previous chat history  \n",
    "- Prompt & get answers from ChatGPT from all these messages  \n",
    "- Return a new ChatMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c803cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Handle the form submission\n",
    "@app.post('/')\n",
    "def send(session, msg: str, contents: list[str] = None, roles: list[str] = None):\n",
    "    if \"session_id\" not in session:\n",
    "        # Initialize tools in session tools and create a session ID\n",
    "        session_id, tools = initialize_handlers(\n",
    "            session_tools=session_tools,\n",
    "            functions=[add_api_tools, add_function_tools, remove_tools], \n",
    "            service_name=\"toolbox_handler\",\n",
    "            fixup=execute_handler\n",
    "        )\n",
    "        session[\"session_id\"] = session_id\n",
    "        tools.extend(YTLiveTools)\n",
    "    else:\n",
    "        # Retrieve toolbox via session ID\n",
    "        session_id = session[\"session_id\"]\n",
    "        tools = session_tools[session_id]\n",
    "\n",
    "    # If no contents or roles are provided, set them to empty lists\n",
    "    if not contents: contents = []\n",
    "    if not roles: roles = []\n",
    "\n",
    "    # Create chat messages from the provided contents and roles\n",
    "    messages = [ form_msg(role, content) for role, content in zip(roles, contents) ]\n",
    "    nof_old_msgs = len(messages) # Number of old messages\n",
    "    messages.append(form_msg(\"user\", msg))\n",
    "    \n",
    "    # Add the user's message to the chat history\n",
    "    complete(messages, tools)\n",
    "    responses = messages[nof_old_msgs:]  # Get only the new messages\n",
    "    \n",
    "    # Create chat messages from the responses\n",
    "    chat_messages = [\n",
    "        ChatMessage(res['content'], res['role'] == 'user') for res in responses if 'content' in res \\\n",
    "            if res['role'] in ['user', 'assistant'] and res['content'] is not None\n",
    "    ]\n",
    "    \n",
    "    return (*chat_messages,\n",
    "            ChatInput()) # And clear the input field via an OOB swap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3dc6cb",
   "metadata": {},
   "source": [
    "## Runner\n",
    "\n",
    "In addition to the main app, an utility function is implemented to run the app just by importing and executing this function to a Python file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a8309b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def llmcam_chatbot(\n",
    "        package_name=\"ninjalabo.llmcam\",  # The installed package name\n",
    "        module_name=\"chat_ui\",  # The module containing the FastAPI app\n",
    "        app_variable=\"app\",  # The FastAPI app variable name\n",
    "        host=\"0.0.0.0\",  # The host to listen on\n",
    "        port=5001,  # The port to listen on\n",
    "        **uvicorn_kwargs  # Additional keyword arguments for uvicorn\n",
    "    ):\n",
    "    \"Find and run the FastAPI app in the specified module within the given package.\"\n",
    "    # Construct the full module path (e.g., 'llmcam.chat_ui')\n",
    "    full_module_path = f\"{package_name.split('.')[-1]}.{module_name}\"\n",
    "\n",
    "    # Check if the module exists in the installed package\n",
    "    try:\n",
    "        spec = importlib.util.find_spec(full_module_path)\n",
    "        if spec is None:\n",
    "            print(f\"Module '{full_module_path}' not found in package '{package_name}'.\")\n",
    "            return\n",
    "        # Dynamically run the Uvicorn server\n",
    "        uvicorn.run(f\"{full_module_path}:{app_variable}\", host=host, port=port, **uvicorn_kwargs)\n",
    "    except Exception as e:\n",
    "        print(f\"Error running the app: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d21e80f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/ninjalabo/llmcam/blob/main/llmcam/chat_ui.py#L155){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### llmcam_chatbot\n",
       "\n",
       ">      llmcam_chatbot (package_name='ninjalabo.llmcam', module_name='chat_ui',\n",
       ">                      app_variable='app', host='0.0.0.0', port=5001,\n",
       ">                      **uvicorn_kwargs)\n",
       "\n",
       "*Find and run the FastAPI app in the specified module within the given package.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| package_name | str | ninjalabo.llmcam | The installed package name |\n",
       "| module_name | str | chat_ui | The module containing the FastAPI app |\n",
       "| app_variable | str | app | The FastAPI app variable name |\n",
       "| host | str | 0.0.0.0 | The host to listen on |\n",
       "| port | int | 5001 | The port to listen on |\n",
       "| uvicorn_kwargs |  |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/ninjalabo/llmcam/blob/main/llmcam/chat_ui.py#L155){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### llmcam_chatbot\n",
       "\n",
       ">      llmcam_chatbot (package_name='ninjalabo.llmcam', module_name='chat_ui',\n",
       ">                      app_variable='app', host='0.0.0.0', port=5001,\n",
       ">                      **uvicorn_kwargs)\n",
       "\n",
       "*Find and run the FastAPI app in the specified module within the given package.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| package_name | str | ninjalabo.llmcam | The installed package name |\n",
       "| module_name | str | chat_ui | The module containing the FastAPI app |\n",
       "| app_variable | str | app | The FastAPI app variable name |\n",
       "| host | str | 0.0.0.0 | The host to listen on |\n",
       "| port | int | 5001 | The port to listen on |\n",
       "| uvicorn_kwargs |  |  |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(llmcam_chatbot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7497fe71",
   "metadata": {},
   "source": [
    "For running while testing with Jupyter notebook, use the `JupyUvi` in `fasthtml` to run in separate thread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2e1b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "from fasthtml.jupyter import *\n",
    "\n",
    "server = JupyUvi(app=app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95a1c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "server.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b67952f-5064-48ad-a0f5-a141be065b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
