{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36f0ff07-13fe-4ebe-970d-4848f7462e09",
   "metadata": {},
   "source": [
    "# chat UI in fastHTML\n",
    "\n",
    "> chat UI implemented in fastHTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b36b977-f531-4d05-af7a-1761ef5d2683",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp chat_ui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd185b2-ec84-46d1-8474-9038ee3eb232",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd17c22c",
   "metadata": {},
   "source": [
    "## Chat App initialization\n",
    "\n",
    "Start by creating the chat application with `FastHTML`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6714c4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import uvicorn\n",
    "import uuid\n",
    "from fasthtml.common import *\n",
    "from typing import Callable\n",
    "\n",
    "from llmcam.fn_to_fc import capture_youtube_live_frame_and_save, ask_gpt4v_about_image_file\n",
    "from llmcam.fn_to_fc import tool_schema, complete, form_msg\n",
    "from llmcam.store import add_api_tools, add_function_tools, remove_tools\n",
    "from llmcam.store import execute_handler_core, handler_schema\n",
    "from llmcam.yolo import detect_objects\n",
    "from llmcam.dtcam import cap\n",
    "from llmcam.file_manager import list_image_files, list_detection_files\n",
    "from llmcam.plotting import plot_object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900cbb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Set up database for storing chat messages per session\n",
    "session_messages = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ab256c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Set up default tools\n",
    "default_tools = [tool_schema(fn) for fn in (\n",
    "    capture_youtube_live_frame_and_save, \n",
    "    ask_gpt4v_about_image_file,\n",
    "    detect_objects,\n",
    "    cap,\n",
    "    list_image_files,\n",
    "    list_detection_files,\n",
    "    plot_object)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4cd069",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Set up database for storing tools per session\n",
    "session_tools = {}\n",
    "\n",
    "# Utility functions to manage tools per session\n",
    "def prepare_handler_schemas(\n",
    "    session_id: str,  # Session ID to use\n",
    "    fixup: Callable = None,  # Optional function to fix up the execution\n",
    "):\n",
    "    return [\n",
    "        handler_schema(function, service_name=\"toolbox_manager\", fixup=fixup, session_id=session_id) for \\\n",
    "        function in [add_api_tools, add_function_tools, remove_tools]\n",
    "    ]\n",
    "\n",
    "def execute_handler(\n",
    "    function_name: str,  # Name of the function to execute\n",
    "    session_id: str,  # Session ID to use\n",
    "    **kwargs,  # Additional arguments to pass to the function\n",
    "):\n",
    "    tools = session_tools[session_id]\n",
    "    return execute_handler_core(tools, function_name, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5b1df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Set up the app, including daisyui and tailwind for the chat component\n",
    "hdrs = (picolink,\n",
    "        Link(rel=\"icon\", href=f\"\"\"{os.getenv(\"LLMCAM_DATA\", \"../data\")}/favicon.ico\"\"\", type=\"image/png\"),\n",
    "        Script(src=\"https://cdn.tailwindcss.com\"),\n",
    "        Link(rel=\"stylesheet\", href=\"https://cdn.jsdelivr.net/npm/daisyui@4.11.1/dist/full.min.css\"),\n",
    "        Style(\"p {color: black;}\"),\n",
    "        Style(\"li {color: black;}\"),\n",
    "        MarkdownJS(), HighlightJS(langs=['python', 'javascript', 'html', 'css']))\n",
    "app = FastHTML(hdrs=hdrs, exts=\"ws\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0211a31",
   "metadata": {},
   "source": [
    "## Chat components\n",
    "\n",
    "Basic chat UI components can include Chat Message and a Chat Input. For a Chat Message, the important attributes are the actual message (str) and the role of the message owner (user - boolean value whether the owner is the user, not the AI assistant)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6c7f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Chat message component (renders a chat bubble)\n",
    "def ChatMessage(\n",
    "        msg: str,  # Message to display\n",
    "        user: bool  # Whether the message is from the user or assistant\n",
    "    ):  # Returns a Div containing the chat bubble\n",
    "    # Set class to change displayed style of bubble\n",
    "    content_class = \"chat-bubble chat-bubble-primary\" if user else \"\"\n",
    "    content_class += \" marked py-2\"\n",
    "    user_style = \"background-color: #fff0c7;\"\n",
    "    return  Div(cls=f\"chat chat-end py-4\" if user else \"py-4\")(\n",
    "                Div('User' if user else 'Assistant', cls=\"chat-header\"),\n",
    "                Div(\n",
    "                    msg,\n",
    "                    cls=content_class,\n",
    "                    style=user_style if user else \"\")\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d96926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/ninjalabo/llmcam/blob/main/llmcam/chat_ui.py#L69){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ChatMessage\n",
       "\n",
       ">      ChatMessage (msg:str, user:bool)\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| msg | str | Message to display |\n",
       "| user | bool | Whether the message is from the user or assistant |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/ninjalabo/llmcam/blob/main/llmcam/chat_ui.py#L69){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ChatMessage\n",
       "\n",
       ">      ChatMessage (msg:str, user:bool)\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| msg | str | Message to display |\n",
       "| user | bool | Whether the message is from the user or assistant |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(ChatMessage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc84d14",
   "metadata": {},
   "source": [
    "For the chat input, set the name for submitting a new message via form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6bbd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# The input field for the user message. Also used to clear the\n",
    "# input field after sending a message via an OOB swap\n",
    "def ChatInput():  # Returns an input field for the user message\n",
    "    return Input(name='msg', id='msg-input', placeholder=\"Type a message\",\n",
    "                 cls=\"input input-bordered w-full rounded-l-2xl\", \n",
    "                 hx_swap_oob='true'  # Re-render the element to remove submitted message\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8a8814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/ninjalabo/llmcam/blob/main/llmcam/chat_ui.py#L90){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ChatInput\n",
       "\n",
       ">      ChatInput ()"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/ninjalabo/llmcam/blob/main/llmcam/chat_ui.py#L90){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ChatInput\n",
       "\n",
       ">      ChatInput ()"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(ChatInput)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7986a51e",
   "metadata": {},
   "source": [
    "### Action Buttons\n",
    "\n",
    "Simple actions for creating a new message from the user side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cba3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def ActionButton(\n",
    "        session_id: str,  # Session ID to use\n",
    "        content: str,  # Text to display on the button\n",
    "        message: str = None  # Message to send when the button is clicked\n",
    "    ):  # Returns a button with the given content\n",
    "\n",
    "    return Form(\n",
    "        ws_send=True,\n",
    "        hx_ext='ws', ws_connect='/wschat',\n",
    "    )(\n",
    "        Hidden(session_id, name=\"session_id\"),\n",
    "        Hidden(content if message is None else message, name=\"msg\"),\n",
    "        Button(\n",
    "            content, \n",
    "            cls=\"btn btn-secondary rounded-2 h-fit\", \n",
    "            style=\"background-color: #ffe485; color: black; border-color: #ffe485;\")\n",
    "    )\n",
    "\n",
    "def ActionPanel(\n",
    "        session_id: str  # Session ID to use\n",
    "    ):  # Returns a panel of action buttons\n",
    "    return Div(\n",
    "        P(\"Quick actions\", cls=\"text-lg text-black\"),\n",
    "        ActionButton(session_id, \"Introduce your model GPT-4o\"),\n",
    "        ActionButton(session_id,\n",
    "            \"Extract information from a YouTube Live\", \n",
    "            \"Capture and extract information from a YouTube Live. Use the default link.\"),\n",
    "        cls=\"flex flex-col h-fit gap-4 py-4 px-4\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe59ea66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/ninjalabo/llmcam/blob/main/llmcam/chat_ui.py#L97){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ActionButton\n",
       "\n",
       ">      ActionButton (session_id:str, content:str, message:str=None)\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| session_id | str |  | Session ID to use |\n",
       "| content | str |  | Text to display on the button |\n",
       "| message | str | None | Message to send when the button is clicked |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/ninjalabo/llmcam/blob/main/llmcam/chat_ui.py#L97){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ActionButton\n",
       "\n",
       ">      ActionButton (session_id:str, content:str, message:str=None)\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| session_id | str |  | Session ID to use |\n",
       "| content | str |  | Text to display on the button |\n",
       "| message | str | None | Message to send when the button is clicked |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(ActionButton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9f47bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/ninjalabo/llmcam/blob/main/llmcam/chat_ui.py#L114){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ActionPanel\n",
       "\n",
       ">      ActionPanel (session_id:str)\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| session_id | str | Session ID to use |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/ninjalabo/llmcam/blob/main/llmcam/chat_ui.py#L114){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ActionPanel\n",
       "\n",
       ">      ActionPanel (session_id:str)\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| session_id | str | Session ID to use |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(ActionPanel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174209ca",
   "metadata": {},
   "source": [
    "## Router"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01b9a76",
   "metadata": {},
   "source": [
    "### Home page\n",
    "The home page should contain our message list and the Chat Input. The main page can be extracted by accessing the index (root) endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de53dced",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "scroll_script = Script(\"\"\"\n",
    "  // Function to scroll to the bottom of an element\n",
    "  function scrollToBottom(element) {\n",
    "    element.scrollTop = element.scrollHeight;\n",
    "  }\n",
    "\n",
    "  // Reference the expanding element\n",
    "  const expandingElement = document.getElementById('chatlist');\n",
    "\n",
    "  // Observe changes to the element's content and scroll down automatically\n",
    "  const observer = new MutationObserver(() => {\n",
    "    scrollToBottom(expandingElement);\n",
    "  });\n",
    "\n",
    "  // Start observing the expanding element for changes\n",
    "  observer.observe(expandingElement, { childList: true, subtree: true });\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61c0cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "title_script = Script(\"\"\"\n",
    "    // Function to set the title of the page\n",
    "    document.title = \"LLMCAM\";\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3371d10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@app.get('/')\n",
    "def index(session):\n",
    "    if \"session_id\" not in session or session[\"session_id\"] not in session_tools:\n",
    "        # Initialize tools in session tools and create a session ID\n",
    "        session_id = str(uuid.uuid4())\n",
    "        session[\"session_id\"] = session_id\n",
    "\n",
    "        # Add default tools and prepare handler schemas\n",
    "        session_tools[session_id] = []\n",
    "        session_tools[session_id].extend(default_tools)\n",
    "        session_tools[session_id].extend(prepare_handler_schemas(session_id, execute_handler))\n",
    "\n",
    "        # Initialize messages in session messages\n",
    "        session_messages[session_id] = []\n",
    "    else:\n",
    "        session_id = session[\"session_id\"]\n",
    "    \n",
    "    sidebar = Div(\n",
    "        ActionPanel(session_id=session_id),\n",
    "        P(\"Conversations\", cls=\"text-lg text-black px-4\"),\n",
    "        cls=\"w-[30vw] flex flex-col p-0\",\n",
    "        style=\"background-color: White;\"\n",
    "    )\n",
    "    page =  Div(cls=\"w-full flex flex-col p-0\")(  # Main page\n",
    "        Form(\n",
    "            ws_send=True,\n",
    "            hx_ext='ws', ws_connect='/wschat',\n",
    "            cls=\"w-full flex flex-col px-24 h-[100vh]\"\n",
    "        )(\n",
    "            Hidden(session_id, name=\"session_id\"),\n",
    "            # The chat list\n",
    "            Div(id=\"chatlist\", cls=\"chat-box overflow-y-auto flex-1 w-full mt-10 p-4\")(\n",
    "                # One initial message from AI assistant\n",
    "                ChatMessage(\"Hello! I'm a chatbot. How can I help you today?\", False),\n",
    "            ),\n",
    "            # Input form\n",
    "            Div(cls=\"h-fit mb-5 mt-5 flex space-x-2 mt-2 p-4\")(\n",
    "                Group(\n",
    "                    ChatInput(), \n",
    "                    Button(\"Send\", cls=\"btn btn-primary rounded-r-2xl\", style=\"background-color: #ffe485;\"))\n",
    "            ),\n",
    "            scroll_script,\n",
    "            title_script\n",
    "        )   \n",
    "    )\n",
    "    return Main(\n",
    "        sidebar,\n",
    "        page, \n",
    "        title=\"Chatbot\",\n",
    "        data_theme=\"lemonade\", \n",
    "        cls=\"h-[100vh] w-full relative flex flex-row items-stretch overflow-hidden transition-colors z-0 p-0\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f987f6",
   "metadata": {},
   "source": [
    "### Chat websockets\n",
    "\n",
    "When connecting to websockets for chat, this function should:\n",
    "\n",
    "- Extract the new and all previous chat history  \n",
    "- Prompt & get answers from ChatGPT from all these messages  \n",
    "- Return a new ChatMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c803cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# The chatbot websocket handler\n",
    "@app.ws('/wschat')\n",
    "async def wschat(msg: str, send, session_id: str):\n",
    "    global session_tools\n",
    "    global execute_handler\n",
    "    \n",
    "    # Create chat messages from the provided contents and roles\n",
    "    messages = session_messages.get(session_id, [])\n",
    "    if len(messages) == 0:\n",
    "        messages.append(form_msg(\"system\", \"You are a helpful assistant. Use the supplied tools to assist the user. \\\n",
    "                      If asked to show or display an image or plot, do it by embedding its path via Markdown syntax.\"))\n",
    "    messages.append(form_msg(\"user\", msg))\n",
    "    await send(\n",
    "        Div(ChatMessage(\n",
    "            messages[-1][\"content\"],\n",
    "            messages[-1][\"role\"] == \"user\"), \n",
    "        hx_swap_oob='beforeend', id=\"chatlist\"))\n",
    "    \n",
    "    await send(ChatInput())  # Clear the input field\n",
    "    \n",
    "    # Add the user's message to the chat history\n",
    "    complete(messages, session_tools[session_id])\n",
    "    await send(Div(ChatMessage(\n",
    "            messages[-1][\"content\"],\n",
    "            messages[-1][\"role\"] == \"user\"), hx_swap_oob='beforeend', id=\"chatlist\"))\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202a6b79",
   "metadata": {},
   "source": [
    "### Static files\n",
    "\n",
    "In case the user needs to display images, serves files from directory `../data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3310edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Serve files from the 'data' directory\n",
    "@app.get(\"/data/{file_name:path}\")\n",
    "def get_file(file_name: str):\n",
    "    \"\"\"Serve files dynamically from the 'data' directory.\"\"\"\n",
    "    data_path = save_dir = os.getenv(\"LLMCAM_DATA\", \"../data\")\n",
    "    file_path = Path(data_path) / file_name\n",
    "    if file_path.exists():\n",
    "        return FileResponse(file_path)\n",
    "    return {\"error\": f\"File '{file_name}' not found\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3dc6cb",
   "metadata": {},
   "source": [
    "## Runner\n",
    "\n",
    "In addition to the main app, an utility function is implemented to run the app just by importing and executing this function to a Python file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a8309b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def llmcam_chatbot(\n",
    "        host=\"0.0.0.0\",  # The host to listen on\n",
    "        port=5001,  # The port to listen on\n",
    "    ):\n",
    "    # Import app from chat_ui base module\n",
    "    from llmcam.chat_ui import app\n",
    "\n",
    "    # Initialize session tools and execute handler\n",
    "    session_tools = {}\n",
    "    globals()[\"session_tools\"] = session_tools\n",
    "\n",
    "    def execute_handler(\n",
    "        function_name: str,  # Name of the function to execute\n",
    "        session_id: str,  # Session ID to use\n",
    "        **kwargs,  # Additional arguments to pass to the function\n",
    "    ):\n",
    "        tools = session_tools[session_id]\n",
    "        return execute_handler_core(tools, function_name, **kwargs)\n",
    "    \n",
    "    globals()[\"execute_handler\"] = execute_handler\n",
    "    \n",
    "    # Run application with uvicorn\n",
    "    uvicorn.run(app, host=host, port=port, log_level=\"info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d21e80f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/ninjalabo/llmcam/blob/main/llmcam/chat_ui.py#L245){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### llmcam_chatbot\n",
       "\n",
       ">      llmcam_chatbot (host='0.0.0.0', port=5001)\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| host | str | 0.0.0.0 | The host to listen on |\n",
       "| port | int | 5001 | The port to listen on |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/ninjalabo/llmcam/blob/main/llmcam/chat_ui.py#L245){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### llmcam_chatbot\n",
       "\n",
       ">      llmcam_chatbot (host='0.0.0.0', port=5001)\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| host | str | 0.0.0.0 | The host to listen on |\n",
       "| port | int | 5001 | The port to listen on |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(llmcam_chatbot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7497fe71",
   "metadata": {},
   "source": [
    "For running while testing with Jupyter notebook, use the `JupyUvi` in `fasthtml` to run in separate thread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2e1b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=LMZQ7eFhm58\n",
      "[youtube] LMZQ7eFhm58: Downloading webpage\n",
      "[youtube] LMZQ7eFhm58: Downloading ios player API JSON\n",
      "[youtube] LMZQ7eFhm58: Downloading mweb player API JSON\n",
      "[youtube] LMZQ7eFhm58: Downloading m3u8 information\n",
      "[youtube] LMZQ7eFhm58: Downloading m3u8 information\n",
      "cap_2024.11.28_12:56:23_unclear.jpg\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=LMZQ7eFhm58\n",
      "[youtube] LMZQ7eFhm58: Downloading webpage\n",
      "[youtube] LMZQ7eFhm58: Downloading ios player API JSON\n",
      "[youtube] LMZQ7eFhm58: Downloading mweb player API JSON\n",
      "[youtube] LMZQ7eFhm58: Downloading m3u8 information\n",
      "[youtube] LMZQ7eFhm58: Downloading m3u8 information\n",
      "cap_2024.11.28_13:01:13_unclear.jpg\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=LMZQ7eFhm58\n",
      "[youtube] LMZQ7eFhm58: Downloading webpage\n",
      "[youtube] LMZQ7eFhm58: Downloading ios player API JSON\n",
      "[youtube] LMZQ7eFhm58: Downloading mweb player API JSON\n",
      "[youtube] LMZQ7eFhm58: Downloading m3u8 information\n",
      "[youtube] LMZQ7eFhm58: Downloading m3u8 information\n",
      "cap_2024.11.28_13:02:58_unclear.jpg\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "from fasthtml.jupyter import *\n",
    "\n",
    "server = JupyUvi(app=app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95a1c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "server.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b67952f-5064-48ad-a0f5-a141be065b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
